<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Notes on Day 2 - The One with Dotnet @Codecamp Romania, 2021</title>
      <link href="2021/04/02/notes-on-day-2-the-one-with-dotnet-codecamp-romania-2021/"/>
      <url>2021/04/02/notes-on-day-2-the-one-with-dotnet-codecamp-romania-2021/</url>
      
        <content type="html"><![CDATA[<p>There was a small talk when Raffaele told us what he did in this corona time. So he started working with ML and also Code Generation driven by ML.</p><p>There was also a talk about how ML was involved in the industry. For example, every machine now starts having more user-friendly interfaces and through this more things can be automated to provide a much greater experience to end customers. And also to the factories that can apply ML to simulate a result before creating and applying those concepts.</p><p><strong>Power your .NET application with the new generation of diagnostics - March 25 14:00 - 14:45 - Raffaele Rialdi</strong></p><p>The session goal was to show how to automate the diagnostic process in production.</p><p>The first step was when to trigger a diagnostic with the DiagnosticClient library. And the second step was that once the data was captured, we can programmatically analyze the process that gave the problem with ClrMD 2.0 library.</p><p>Preemptive profiling or microbenchmark is used on dev machines to analyze specific lines of code that are not performing as expected, by using Benchmark.NET.</p><p>In production, the primary source of help are the logs. But those logs should not be too verbose but also not too short, for example containing too little information.</p><p>A dump can be a solution by using for example dotnet-dump, but it is not a tool for investigating problems, because the problem can be done after analyzing it, and catching the perfect moment is hard.</p><p>In production, the performance can be monitored with dotnet-trace, dotnet-counter and in .NET 5 we can use a new tool dotnet-monitor that can connect to remotely located and locally we can access it through a web interface.</p><p>There is also another problem with dumps because there might be sensitive data, for example, the application could be installed in a hospital, so those data can be constrained by privacy and EU regulation like GDPR.</p><p>The presented scenario was an <a href="http://asp.net/">ASP.NET</a> Core application and many clients that use the app. There is also a console application used to stress the web service, that can make concurrent requests. The diagnostic application is receiving TraceEvents from the web service. This application uses two libraries: DiagnosticClient and ClrMD 2.0 (the investigating library, that grabs data). When an issue occurs a snapshot with the web service is created by this diagnostic app. By doing so we can capture the exact moment when the problem occurred.</p><p>The communication between web service and diag app is the communication that can be made with every .NET application starting with .NET Core 3.0.</p><p>The library recommended is <a href="https://github.com/raffaeler/PowerDiagnostics">https://github.com/raffaeler/PowerDiagnostics</a>. </p><p>Diagnostic Demo app was used to capture snapshots and also run different queries to display the root cause of the problem. For example, discovering a big byte array that uses too much memory. Or querying duplicate strings, getting strings by size, list of modules or threads stacks.</p><p>The app can also load a dump saved before and that dump can be investigated as dump saved at the runtime.</p><p>.NET 5 introduces reversed communication protocol, which can be used to diagnose apps, like subscribing to them and when an event occurs, the diagnosis app can read them.</p><p><strong>The (too) Many Faces of Architecture - March 25 15:00 - 15:45 - Mihaela Ghidersa</strong></p><p>This presentation was a generic one about architecture, discussing different types of architecture.</p><p>Architecture is a technical solution that makes artistic or more abstract concepts materialize.</p><p>In the real world, architecture is creating a small module and combining both technical and business concepts into a big schema or structure.</p><p>The architecture can be seen as a blueprint. We should define what and why is significant.</p><p>The architect is not about the title, he or she has a flexible role, that can collaborate more than giving indications.</p>    <div>      <img src="Untitled.png" alt="" data-action="zoom" class="photozoom">          </div><p><strong>Telemetry in .NET distributed applications - March 25 16:00 - 16:45 - Constantin-Ariton Lazar  /  Mihai Detesan</strong></p><p>The presentation was focusing on automotive telemetry by using Bosch sensors. </p><p>There was a comparison between the Monolith application and the new way of developing products with microservices.</p><p>One of a problem with microservices is with the coherence, which can be difficult to track because each microservice creates request between them. And for example, if an error occurred is a bit hard to know from where it comes.</p><p>And also another problem comes from debugging and is also difficult to locate.</p><p>In monolith application scaling represents adding more hardware resources, instead of on microservices scaling represents adding more instances of a specific or many microservices.</p><p>The “telemetry” word comes from Greek, tele = distance and metron = measure. The “sum” of this to words can be translated as a process of recording and transmitting some logs or reading of a sensor or even a machine.</p><p>OpenTelemtry is an observability framework that combines traces and metrics.</p><p>Jaeger and Zipkin were used for tracing.</p><p>To trace all microservices, each of them has a Trace Id that uniquely identifies each microservice.</p><p>The trace collector creates some sort of graph from all traces.</p><p>All the Trace IDs are sent from a microservice to another when creating a request, to know which microservice called another one.</p><p>The final id is composed of the Trace Id and the Parent Id.</p><p>Elasticsearch was used to store the logs with Timestamp, Service context like IP or the microservice name, the operation context like the stack trace, and the Trace Id.</p><p><strong>What’s New in C# - March 25 17:00 - 17:45 - Chris Sienkiewicz  /  Fred Silberberg</strong></p><p>In C# there are top-level statements, before declaring usings, namespace, and declaring classes. And those lines are executed. The compiler will take it and put it in a form to execute it.<br>Even async calls will work in top-level statements because the main method that runs these lines is an async one.<br>For not including the namespace in the statements we can use using before them.<br>In top-level statements functions can also be declared. But the functions can be used only in the local top statements. So for example, if there are classes under, those top functions can’t be called in. </p>    <div>      <img src="Untitled1.png" alt="" data-action="zoom" class="photozoom">          </div><p>Because var fields can’t be used for declaring fields at the class level. Now those fields can be instantiated only by calling <em>new()</em>.</p>    <div>      <img src="Untitled2.png" alt="" data-action="zoom" class="photozoom">          </div><p>The new keyword can also be used to instantiate a class without specifying the name.</p>    <div>      <img src="Untitled3.png" alt="" data-action="zoom" class="photozoom">          </div><p>To avoid overriding the not equal operator (!=) when checking for null, we can verify an object that is not null, by using exactly those keywords “is not null”.</p>    <div>      <img src="Untitled4.png" alt="" data-action="zoom" class="photozoom">          </div><p>When comparing two objects to avoid overriding the Equals method, the class can be changed to a record, that represents a class type with value semantics, compares equality by value. This works only to type fields.<br>A record can be seen as a collection of values.</p><p>There is a new future called initialization only available only to <em>record</em>, which provides the ability to assign a property only once in the object initialization.</p><p>The <em>with</em> keyword provides non-destructive mutation, that creates a copy of the variable. So values can be set as a part of the initialization.</p>    <div>      <img src="Untitled5.png" alt="" data-action="zoom" class="photozoom">          </div><p>Records offer the possibility to add a property at the declaration level, similar to the constructor.</p>    <div>      <img src="Untitled6.png" alt="" data-action="zoom" class="photozoom">          </div><p>And the inheritance looks very similar to the constructor, without using the base keyword.</p>    <div>      <img src="Untitled7.png" alt="" data-action="zoom" class="photozoom">          </div><p>In C# 10 there can be some of the following futures:</p><p>The namespaces can be declared without braces.</p><p>The global keyword can be used with using to declare a variable or a namespace in a single place. This will make those declarations available all over other files.</p><p>Nullable types can be used in if.</p><p>There can be semi-auto properties that have a backing field, that doesn’t need to be declared.</p><p>The records can be also structs (record struct), similar to the current record for classes.</p><p>There can be required properties that uses the <em>required</em> keyword that requires the user to set it. </p><p>The IAddable interface can be used to “override” the plus sign to perform add operation with which type is needed. </p><p><strong>Recipe for Modern Applications: .NET, Azure SQL, Functions, Geospatial, JSON - March 25 18:00 - 18:45 - Anna Hoffman</strong></p><p>The presentation was about creating an application to be notified when a bus arrives.</p><p>The data are imported from the public transportation data. Azure was used for all the below “pieces”.</p>    <div>      <img src="Untitled8.png" alt="" data-action="zoom" class="photozoom">          </div><p>For the database, Azure SQL Database was used because it has the multi-model capability and also native geospatial support. And another important point of using Azure SQL Database is the feature of using JSON because the conversion is done automatically (from non-relational to relational).</p><p>Azure Data Studio looks very similar to Jupiter used for Python. It is a notebook where both code (SQL, Powershell) and documentation can be written.</p><p>There are geofences created to be notified when the bus enters them.</p><p>Overall, this was my first Code camp conference and also my first conference online. It was a nice experience, but it is not compared with a real-life conference, where you can see the presenters and even ask them questions after the session.<br>But if the corona still stays among us, these conferences are a great way to refresh your mind with what’s new in technology and in .NET.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnet </tag>
            
            <tag> conference </tag>
            
            <tag> codecamp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes on Day 1 - The One with Dotnet @Codecamp Romania, 2021</title>
      <link href="2021/03/26/notes-on-day-1-the-one-with-dotnet-codecamp-romania-2021/"/>
      <url>2021/03/26/notes-on-day-1-the-one-with-dotnet-codecamp-romania-2021/</url>
      
        <content type="html"><![CDATA[<h2 id="13-45-14-00"><a href="#13-45-14-00" class="headerlink" title="13:45 - 14:00"></a>13:45 - 14:00</h2><p>The first conference day started with Dino Esposito discussing that it matters more the soft skills and the pragmatic way to approach problems rather than the used technology, like .NET.<br>In a sports business, the money comes from the data, that is applied to the sportsmen and women. All the things that we are seeing on TV look so simple, but all of them are based on data. “Data is gold”.</p><p>Data Science is only an umbrella because we need engineering to apply the science and make it available. This definition of Science fits Machine Learning and all those technologies related to it.</p><h2 id="Data-Engineering-vs-Data-Science-March-24-14-00-14-45-Dino-Esposito"><a href="#Data-Engineering-vs-Data-Science-March-24-14-00-14-45-Dino-Esposito" class="headerlink" title="(Data) Engineering vs (Data) Science - March 24 14:00 - 14:45 -  Dino Esposito"></a>(Data) Engineering vs (Data) Science - March 24 14:00 - 14:45 -  Dino Esposito</h2><ul><li>Data Science is a starting point, and Data Engineering is the final point.</li><li>Science demonstrates what can be done, and Engineering shows how that works.</li><li>AI is considered to be just software, but it can become intelligent.</li><li>Software that we write today is not as intelligent as ML one.</li><li>The Data Science Teams is working with the dataset.</li><li>Most of the time the algorithm is all right, the problem might be with the web service as a link in a chain that might be broken.</li><li>The role of Data Engineer override with DS and ML Developer. And that is because that guarantees raw data is captured and in the right way to be processed and become a product.</li><li>Data Scientist used techniques from math and stats to view through the raw data available. It can be compared with a sculptor, that takes out marble pieces to make a beautiful sculpture. It might have a view before processing (sculpting) and through the data. The DS should tell the company what can be learned from the data and what can be planned.</li><li>Data Scientist can be viewed as a Product Owner, but for data. Data Scientist should have advanced knowledge in abstract math.</li><li>The company has a team of DS that think can be a great asset with the risk of producing nothing real but coming with ideas.</li><li>In history the Data scientists were done by most of the big kingdom to support expeditions in unknown territories, in the hope of getting a financial return.</li><li>The DE is not expected to have a deep knowledge of math but should have deep knowledge of databases. For example, how to move big data, from Data Lake to Data Warehouse. Should also have skills of how to automate those processes. Should also know knowing how often the expected model should be re-trained to stay adherent to close real data.</li><li>Companies to improve applications need real-world workloads to be scalable enough.</li><li>A role of DS is not enough, the company also needs DE and infrastructure.</li><li>A role is a set of skills of an individual.</li><li>The professional is also about an individual but has the skills to cover a given role.</li><li>The ML Engineer should write good software, applying best practices, and being comfortable with web services technologies like API or gRPC. The ML engineer should also incorporate a trained model into a client application.</li><li>Two interesting ideas about how knowledge can be spread:<ul><li>T-shaped knowledge is a deep vertical in one area and basic knowledge of several other side areas.</li><li>Y-shaped is a deep vertical in one area and good knowledge good enough in a small number of side areas. </li></ul></li><li>ML.NET comes as a framework to be helpful to data engineers and ML engineers rather than data scientists. ML.NET can manage up to 1TB of data. Is a tool that helps to integrate ML solution inside of existing or new .NET application. ML.NET is not a Microsoft version of Python is a tool for Data Scientists and Data Engineers.</li></ul><h2 id="Single-tenant-to-multi-tenant-a-real-world-example-March-24-15-00-15-45-Iris-Classon"><a href="#Single-tenant-to-multi-tenant-a-real-world-example-March-24-15-00-15-45-Iris-Classon" class="headerlink" title="Single-tenant to multi-tenant, a real world example - March 24 15:00 - 15:45 - Iris Classon"></a>Single-tenant to multi-tenant, a real world example - March 24 15:00 - 15:45 - Iris Classon</h2><ul><li>Nice presentation slides, drawn by her.</li></ul><p>Sneak peek:</p>    <div>      <img src="tenant.png" alt="" data-action="zoom" class="photozoom">          </div><ul><li>Was a discussion about multiple clients that access a load balancer, which redirects them to a cluster that contains an app, and that app shares multiple databases (tenants) to each client.</li><li>The old version of this was by creating an app for each client, that involves too many resources.</li><li>The new architecture looks something like in the above picture.</li><li>The problem with this architecture is that it should be too many instances of the Web API and also the deployment takes half a week. The deployment for aproximetely 120 customers takes 2 to 3 hours.</li><li>To being fixing that problem, she started with the Web API by using multiple tenants. The tenant can be identified by the subdomain, the domain, or a path.</li><li>The later you do it, the easier it is to get it wrong.</li><li>On a previous company all that logic was on-premises and then to a local cloud provider. But now on the current company, the application became tenant agnostic that runs on the cloud and also has a load balancer.</li></ul><p>The pros:</p><ul><li>Cost saving for scaling.</li><li>Simplified and faster deploying process.</li><li>Logs should be treated differently because we’re not going to have logs per customer.</li></ul><p>The cons:</p><ul><li>Clients are not isolated, so the connection should be encrypted.</li></ul><h2 id="Trailblazor-Building-Dynamic-Applications-with-Blazor-March-24-16-00-16-45-Shaun-Walker"><a href="#Trailblazor-Building-Dynamic-Applications-with-Blazor-March-24-16-00-16-45-Shaun-Walker" class="headerlink" title="Trailblazor: Building Dynamic Applications with Blazor - March 24 16:00 - 16:45 - Shaun Walker"></a>Trailblazor: Building Dynamic Applications with Blazor - March 24 16:00 - 16:45 - Shaun Walker</h2><ul><li>Dynamic applications are also called plugin applications, modular applications, or dynamic applications.</li><li>The benefits are the extensibility of being loosely coupled, which can be developed in parallel.</li><li>Blazor provides a consistent set of technologies to build both backend and frontend.</li><li>A Razor Component has capabilities such as routing, parameters, binding, and life cycle events.</li><li>The web app in the Blazor Server and the communication with the DOM (Browser) is done with SignalR.</li><li>The newer model done in <a href="http://asp.net/">ASP.NET</a> Core, is done by using Web Assembly, and all that code is run in the browser.</li><li>There is an open source app framework build on Blazor called Oqtane. Can be used to create modern web applications with less called. (boilerplates). “Rocket Fuel for Blazor”</li><li>The benefits and the features of Oqtane Framework:</li></ul>    <div>      <img src="blazor.png" alt="" data-action="zoom" class="photozoom">          </div><ul><li>At first look, the razor code looks like aspx code but doesn’t have a .cs file. As far as I understood a partial class can be created to move the code from the razor page.</li><li>The Oqtane should be configured like a WordPress site before using it. The configuration is simple, and after that, the administrator is redirected to the admin page, where settings can be made.</li><li>Even the application looks more like WordPress because content can be added from within the website. This a high-level comparison because the website is using Web Assembly so it has much more capabilities than WordPress.</li><li>Also modules can be created in code, deployed to Oqtane, and then added to the website via admin panel. When creating a module from inside the app, a whole solution is created locally on the server machine, with Client, Server, Package all the projects need for the specific module.</li><li>Oqtane is WordPress on extraterrestrial steroids. At first look is so powerful, and a much better comparison it would be with Oracle Apex, but with total control.</li></ul><h2 id="Moving-a-20-year-old-NET-on-Windows-blog-to-Linux-Containers-and-Azure-March-24-17-00-17-45-Scott-Hanselman"><a href="#Moving-a-20-year-old-NET-on-Windows-blog-to-Linux-Containers-and-Azure-March-24-17-00-17-45-Scott-Hanselman" class="headerlink" title="Moving a 20 year old .NET on Windows blog to Linux, Containers and Azure - March 24 17:00 - 17:45 - Scott Hanselman"></a>Moving a 20 year old .NET on Windows blog to Linux, Containers and Azure - March 24 17:00 - 17:45 - Scott Hanselman</h2><ul><li>Scott start presenting his website. He started blogging in 2002 with .NET 2.0 and then upgraded to .NET 4.x in a Windows Server.</li><li>Because it was hosted on a Windows Server, he needed to login in with RDP for troubleshooting. At that time this Windows Server was hosted, but not a VM on a physical server.</li><li>One of the requirements when moving to another framework was to not break the URLs.</li><li>There is a .NET Portability Analyzer that helps us find code that can’t be moved to a new .NET framework.</li><li>Dotnet try-convert can be used to try to convert a .NET framework project from an old version to a newer one.</li><li>Scott shows how to build the same application in Windows and Ubuntu (WSL) and .NET 5.</li><li>Also some scripts can deploy the blog to Docker containers and can be also hosted to Azure or any cloud provider.</li><li>Azure Front Doors is similar to Cloudflare, which can take different domains and map them to a different backend.</li><li>His /blog is a different Azure App, and this redirect can be done within the same site with Azure Front Doors. Also, the certificate can be owned but managed by Azure.</li><li>The breakdown of the virtual machines was done systematically to not break the website.</li><li>Also by using .NET the website can be moved to any cloud provider.</li></ul><h2 id="Architecture-–-The-Stuff-That’s-Hard-To-Change-March-24-18-00-18-45-Dylan-Beattie"><a href="#Architecture-–-The-Stuff-That’s-Hard-To-Change-March-24-18-00-18-45-Dylan-Beattie" class="headerlink" title="Architecture – The Stuff That’s Hard To Change - March 24 18:00 - 18:45 - Dylan Beattie"></a>Architecture – The Stuff That’s Hard To Change - March 24 18:00 - 18:45 - Dylan Beattie</h2><ul><li>Fred Brooks discussed computer architecture in 1962.</li><li>The stuff that is hard to change are those components that were built wrong and cost money to change them or a part of them.</li></ul>    <div>      <img src="architecture.png" alt="" data-action="zoom" class="photozoom">          </div><ul><li>The architect should look forward and see what’s coming.</li><li>To understand software, we should not look at the code because it is time-consuming. We should look at the traffic pattern, such as HTTP requests, load balancing, how many requests supports.</li><li>We need to ask the right question to be turned into meaningful metrics. For example knowing who to protect, for multiple-factor authentication.</li><li>Before reinventing the wheel search for an existing library, for example from nuget.</li><li>Diagrams should be clear, showing exactly what each of the entries should look like. Add more details to the diagrams to be easily understood. </li><li>I recommend following Dylan Beattie on Twitter (<a href="https://twitter.com/dylanbeattie">https://twitter.com/dylanbeattie</a>) and the conferences he is attending.</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> dotnet </tag>
            
            <tag> conference </tag>
            
            <tag> codecamp </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes on Sandro Mancuso — Software Craftsmanship @Talk</title>
      <link href="2020/05/05/notes-on-sandro-mancuso-software-craftsmanship-talk/"/>
      <url>2020/05/05/notes-on-sandro-mancuso-software-craftsmanship-talk/</url>
      
        <content type="html"><![CDATA[<p>Talk: <a href="https://www.youtube.com/watch?v=9OhXqBlCmrM">https://www.youtube.com/watch?v=9OhXqBlCmrM</a></p><hr><h2 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h2><ul><li>The attitude of software craftsmanship should be: passion, career ownership, “perfect” practice and boy scout rule;</li><li>We should produce more to receive more, and not waiting to receive something from our company because we are owning our career;</li><li>When we are working on an organization don’t excuse, like “No one does that, so I’m not going to do it also”. Either you do it or you leave the company;</li><li>A good developer is not Java, C# or Ruby developer. A good developer is a developer. We use the tools that we find useful for our job;</li></ul><hr><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><ul><li>In February 2001, 17 people meet in a ski resort in Utah to create an Agile Methodology. The people there were people like Uncle Bob, Martin Fowler;</li><li>In agile processes and tools became more important than technical practices and excellence;</li><li>In the beginning there, were more “shits” scrum masters than developers. Some of those scrum masters know nothing about software development;</li><li>Without agile processes there will be no good delivered software;</li><li>“Insanity: doing the same thing over and over again, and expecting different results” — Albert Einstein;</li><li>When we are under pressure we tend to cut corners, by thinking that we deliver faster, even when the managers do not push us;</li><li>We think that we don’t have, but we do. Is our fault that we think that we don’t have, and by doing so, we are delivering low-quality products;</li><li>Agile is about shorting the feedback loop;</li><li>By reacting to that feedback is what gives us the agility;</li><li>Agility is reacting constantly to feedback;</li><li>We don’t do agile, because it’s not logical. We are agile;</li><li>The most important deliver is the software itself;</li><li>To have good software, we need to have a good process;</li><li>Businesses are hostages of their software;</li><li>The business will move as fast as we developers create or change software;</li><li>As the software grows and no attention paid to the quality of it, the thing is that will become much harder;</li><li>In agile we are splitting stories, but delivering all the user stories doesn’t mean that the delivered software is it the required one or at the quality that we want;</li><li>Every bug found by the QA team, is something we developers haven’t done; QA should contribute in defining user stories and read newspapers;</li><li>We don’t want software that breaks, we want software that makes us happy to work with;</li><li>Software that makes us happy is not about beautiful code, but instead is easy to maintain, to change and to test;</li><li>For him, the software is an asset, a huge investment that we need to care about because costs a fortune;</li><li>It doesn’t matter the contract type, the relationship shouldn’t be employer and employee, it should be a partnership;</li><li>We are not keeping the heads down and doing what we are told as in a factory because we are creative people;</li><li>Every craftsman or professional that are working on a project, he or she should strive to make it a success;</li><li>Software craftsmanship in his opinion is professionalism in software development;</li><li>Feedback in software development is based on XP Practices;</li><li>We should not need the authorization to use practices that improve the quality of the product, like Unit Tests, we should just do it;</li><li>Managers are interested in value, we are adding value through practices like: TDD, Continuous Integration, Refactoring, Pair Programming, and Automated Tests;</li><li>When we are talking about business, we should not discuss practices, instead, we should discuss about the value;</li><li>On a new project, at the beginning we will be slower until we will be better and faster;</li><li>Quality depends on who is doing the job, not the time or money;</li><li>“Developers are not football players” and should not cost millions of euros to create quality software;</li><li>Quality cannot be measure and defined precisely because there are so many variables, it means different people, in different places and different points in time;</li><li>Code coverage is a dangerous metric and should never be used as a target or management tool because the percentage of tested code is not the same as the percentage of the executed code. For example, we can have 80% tested code from a 50% executed code;</li><li>“How it is done is as important as having it done” — Eduardo Namur;</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>JWT Token Claims in ASP.NET Core</title>
      <link href="2020/05/02/jwt-token-claims-in-asp-dotnet-core/"/>
      <url>2020/05/02/jwt-token-claims-in-asp-dotnet-core/</url>
      
        <content type="html"><![CDATA[<p>After the authentication was presented in the previous two articles using Symmetric and Asymmetric keys, then this article is about authentication, much more exactly about Claims and Roles. In the first part, there will be an introduction about the core concepts of the JWT Claims and in the second part the actual implementation.</p><p>JWT Authentication with Symmetric Key: <span style="word-break: break-all;"><a href="https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/">https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/</a></span>.<br>JWT Authentication with Asymmetric Key: <span style="word-break: break-all;"><a href="https://stefanescueduard.github.io/2020/04/25/jwt-authentication-with-asymmetric-encryption-in-asp-dotnet-core/">https://stefanescueduard.github.io/2020/04/25/jwt-authentication-with-asymmetric-encryption-in-asp-dotnet-core/</a></span>.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Claims in JWT Token are used to store key data (e.g. username, timezone, or roles) in the Token payload, besides the IssuedAt (i.e. iat), which is added by default.<br>In .NET Core, Claims can be used without installing any additional package, it comes from the <code>System.Security.Claims</code> package. From this package, in this article, just the <code>Claim</code> and <code>ClaimTypes</code> will be used. You can find more about them here: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.security.claims?view=netcore-3.1">https://docs.microsoft.com/en-us/dotnet/api/system.security.claims?view=netcore-3.1</a>.<br>For this article I chose to use <code>JwtAuthentication.AsymmetricEncryption</code> project  from the previous article and to add some functionality to support Claims and Roles. So if you are reading the previous two articles, you’ll see small changes in this one.</p><h2 id="Additional-changes"><a href="#Additional-changes" class="headerlink" title="Additional changes"></a>Additional changes</h2><p>As I said there will be some minor changes, to support the Claims and Roles feature. These changes are not required in your type of scenario, but is required for a better understanding of this article. So if your target is to find the actual implementation, you can skip the <code>AuthenticationService</code> class.</p><h2 id="AuthenticationService"><a href="#AuthenticationService" class="headerlink" title="AuthenticationService"></a><code>AuthenticationService</code></h2><p>The <code>AuthenticationService</code> now will have an additional <code>UserRepository</code> from which the data about the <code>User</code> will be retrieved. And the <code>TokenService</code> will receive the <code>User</code> to generate the <code>securityToken</code>.</p><script src="https://gist.github.com/StefanescuEduard/f7973a0f2d82e93249cc31670f0c2906.js"></script><p><code>Authenticate</code> method was explained in the previous two articles and all the code can be found at my GitHub account, there will be a link to it at the end of this article.<br><code>UserRepository</code> contains a predefined list of users, and the <code>GetUser</code> method returns only the <code>User</code> with the given username, this logic was on the <code>UserService</code>.</p><h2 id="User"><a href="#User" class="headerlink" title="User"></a><code>User</code></h2><p><code>User</code> now contains the <code>Roles</code> property and the <code>Claims</code> method which will build the claims with the <code>Username</code> and <code>Roles</code>. For the sake of this article, we’re supposing that the <code>Roles</code> will be all the time set, so we’ll don’t need to worry if this collection will be <code>null</code>.</p><script src="https://gist.github.com/StefanescuEduard/f95a86f622356375bf35facb0aa05ddd.js"></script><p>You may notice that there are some predefined <code>ClaimTypes</code>, created by a standard (i.e. <a href="http://docs.oasis-open.org/imi/ns/identity-200810">http://docs.oasis-open.org/imi/ns/identity-200810</a>), but there are just plain text. So the <code>ClaimTypes</code>, can be also customized as you wish.<br>The <code>Claims</code> will be used on the <code>TokenService</code> to set the Subject, which in fact is the Token Payload.</p><h2 id="TokenService"><a href="#TokenService" class="headerlink" title="TokenService"></a><code>TokenService</code></h2><p><code>TokenService</code> is receiving the <code>User</code> from the <code>AuthenticationService</code> and uses it to set the Subject (i.e. Payload) of the Token.<br>Besides this change, there is only one change that has to be done, on the <code>GetTokenDescriptor</code> method, when the <code>SecurityTokenDescriptor</code> is created, the subject is initialized with new <code>ClaimsIdentity</code> that gets the user claims.</p><script src="https://gist.github.com/StefanescuEduard/5b7203ac73a10e41b1b7cc1aaa9eb268.js"></script><h2 id="UserController"><a href="#UserController" class="headerlink" title="UserController"></a><code>UserController</code></h2><p>Now that the Claims are set, the <code>UserController</code> will be the playground for the set claims and roles. In order to accept requests with the created Token, the Controller must have the same Scheme as the Token set on the <code>AuthorizeAttribute</code>.</p><script src="https://gist.github.com/StefanescuEduard/011cb08e3d4d77aa4ec849e096a3b89a.js"></script><h3 id="GetClaims-method"><a href="#GetClaims-method" class="headerlink" title="GetClaims method"></a><code>GetClaims</code> method</h3><p>Firstly the user claims will be getting by using the <code>User</code> from the base class  of the controller (i.e. <code>ControllerBase</code>), which has the <code>Claims</code> getter. Because the <code>Claim</code> class has many properties, that can be found on the Microsoft website: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.security.claims.claim?view=netcore-3.1">https://docs.microsoft.com/en-us/dotnet/api/system.security.claims.claim?view=netcore-3.1</a>, for this example just the <code>Type</code> and the <code>Value</code> associated with will be used.</p><script src="https://gist.github.com/StefanescuEduard/6639e28b64bec22208c08e709977e891.js"></script><p>In the picture below, the claims of the john.doe user are get. We can see that besides the <code>name</code> and <code>role</code> claims, there are three more which are not added explicitly; but were added by default when the Token was created.</p>    <div>      <img src="jwt-get-claims.png" alt="" data-action="zoom" class="photozoom">          </div><ul><li><strong><code>nbf</code></strong> or Not Before, is used to verify that token will be valid only after it was created and not in the past;</li><li><strong><code>exp</code></strong> or Expiration Time, it’s self-explanatory and was set because the <code>LifeTimeValidator</code> was specified when the Token was created;</li><li><strong><code>iat</code></strong> or Issued At as previously mentioned, is the time when the Token was created;<br>The time represents the seconds in the Unix epoch time.</li></ul><p>All the claims can be found on this scientific paper, that I used it as a reference for these articles: <a href="https://tools.ietf.org/html/rfc7519#section-4.1">https://tools.ietf.org/html/rfc7519#section-4.1</a>.</p><h3 id="GetName"><a href="#GetName" class="headerlink" title="GetName"></a><code>GetName</code></h3><p>In the <code>GetName</code> method, the value of <code>Name</code> claim is get for the given Token, which represents the username. The <code>User</code> already has predefined methods, like <code>FindFirstValue</code> in order to expose its property easily.</p><script src="https://gist.github.com/StefanescuEduard/af31cc48549c79b7d6eae05417901c8a.js"></script><p>In the response, only the username is returned from the Claim.</p>    <div>      <img src="jwt-get-name.png" alt="" data-action="zoom" class="photozoom">          </div><h3 id="GetRoles"><a href="#GetRoles" class="headerlink" title="GetRoles"></a><code>GetRoles</code></h3><p>And the last method, is using the <code>AuthorizedAttribute</code> with the <code>Roles</code> property to give access only for the users that have the set role, in this case, Admin.</p><script src="https://gist.github.com/StefanescuEduard/39b72af6eb6e6a31028a10591009c227.js"></script><p>Let’s test with the john.doe user, that only have the User role.</p>    <div>      <img src="jwt-get-roles-forbidden.png" alt="" data-action="zoom" class="photozoom">          </div><p>The response code is 403 Forbidden because the request didn’t pass AuthorizeAttribute`.</p><p>Now, the jane.doe user will be logged in, and we’ll try to get her roles with the generated token.</p>    <div>      <img src="jwt-get-roles-ok.png" alt="" data-action="zoom" class="photozoom">          </div><p>In the above picture, the response code is OK and its body contains the user roles, as expected because the role is the requested one.</p><hr><p>The source code from this article can be found on my GitHub account: <a href="https://github.com/StefanescuEduard/JwtAuthentication">https://github.com/StefanescuEduard/JwtAuthentication</a>.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> asp </tag>
            
            <tag> authorization </tag>
            
            <tag> claims </tag>
            
            <tag> roles </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JWT Authentication with Asymmetric Encryption using certificates in ASP.NET Core</title>
      <link href="2020/04/25/jwt-authentication-with-asymmetric-encryption-in-asp-dotnet-core/"/>
      <url>2020/04/25/jwt-authentication-with-asymmetric-encryption-in-asp-dotnet-core/</url>
      
        <content type="html"><![CDATA[<p>In the previous article I wrote about JWT Authentication using a single security key, this being called Symmetric Encryption. The main disadvantage of using this encryption type is that anyone that has access to the key that the token was encrypted with, can also decrypt it. Instead, this article will cover the Asymmetric Encryption for JWT Token.<br>In the first part of this article, the Asymmetric Encryption concept will be explained, and in the second part, there will be the implementation of the JWT Token-based Authentication using the Asymmetric Encryption approach by creating an “Authentication” Provider in ASP.NET Core.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The JWT Token concepts were explained in the previous article, so if you want to find more before continuing reading this article, check out the introduction of the previous one: <a href="https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/#Introduction">https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/#Introduction</a>.<br>Asymmetric Encryption is based on two keys, a public key, and a private key. The public key is used to encrypt, in this case, the JWT Token. And the private key is used to decrypt the received Token. Maybe the previous statement is a little bit fuzzy, but I hope that will make sense in a moment.</p>    <div>      <img src="asymmetric-key.svg" alt="" data-action="zoom" class="photozoom">          </div><p>For using the Asymmetric Encryption, two keys have to be generated, these two keys have to come from the same root. In this case for this article there will be a certificate &ndash;&ndash; the root &ndash;&ndash; from which the private and the public key will be generated. These keys will be also certificates, so the first thing that has to be done is to generate the private certificate &ndash;&ndash; key &ndash;&ndash; and the second one to generate the public certificate &ndash;&ndash; key &ndash;&ndash; from the private certificate.</p><h2 id="Generating-the-keys"><a href="#Generating-the-keys" class="headerlink" title="Generating the keys"></a>Generating the keys</h2><p>To generate certificates I chose to use the OpenSSL toolkit. If you are on Windows, OpenSSL can be downloaded as an executable and installed where ever you want. I recommend being installed on the C:\ root.<br>OpenSSL download link: <a href="https://slproweb.com/products/Win32OpenSSL.html">https://slproweb.com/products/Win32OpenSSL.html</a><br>The tool has to be used from the Terminal, so there are two choices:</p><ol><li>Run the executable from where the tool was installed.</li><li>Add an environment variable to have access to it from everywhere as a CLI.</li></ol><p>To add the tool as an environment variable the following entry has to be inserted to the User variables:</p><pre>  Variable name: OPENSSL_CONF  Variable value: &lt;PATH_TO_OPEN_SSL&gt;\bin\cnf\openssl.cnf</pre><p>After configuring OpenSSL, the private and public key have to be generated using the following commands:</p><ul><li><p>For the private key: <code>openssl genpkey -algorithm RSA -out private_key.pem -pkeyopt rsa_keygen_bits:2048</code></p><ul><li><code>genpkey</code> specifying that we’ll generate a private key;</li><li><code>-algorithm RSA</code> the algorithm used, in this case RSA;</li><li><code>-out private_key.pem</code> the output argument and path;</li><li><code>-pkeyopt rsa_keygen_bits:2048</code> set the public key algorithm and the key size;</li></ul></li><li><p>For the public key: <code>openssl rsa -pubout -in private_key.pem -out public_key.pem</code></p><ul><li><code>rsa</code> specifying that the command will process RSA keys;</li><li><code>-pubout -in private_key.pem</code> the private key and the path of it;</li><li><code>-out public_key.pem</code> the output argument and path;</li></ul><p>Before starting into code, the generated PEM keys have to be converted into XML files. That was the easiest way to read them using the <code>System.Security.Cryptography</code> package.<br>To convert them into XML you can use this site: <a href="https://superdry.apphb.com/tools/online-rsa-key-converter">https://superdry.apphb.com/tools/online-rsa-key-converter</a>, then copy the converted text into two files with the XML extension in the project folder.</p></li></ul><p>The Setup is the same as in the previous article, so check it out here: <a href="https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/#Setup">https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/#Setup</a>. TL;DR you have to install the following package: <code>Microsoft.AspNetCore.Authentication.JwtBearer</code>.</p><h2 id="Startup"><a href="#Startup" class="headerlink" title="Startup"></a><code>Startup</code></h2><p>As in the previous article, the Authentication service has to be added in the <code>ConfigureServices</code> method from the <code>Startup</code> class. For Authentication, an extension method called <code>AddAsymmetricAuthentication</code> will setup the service with the basic settings.<br>It may be a little bit confusing to switch between this and the previous article, but the only thing that is changed here compared to the previous article is the <code>IssuerSigningKey</code> property, which now receives the <code>SigningKey</code>. The previous article contains a comprehensive explanation of each property that it’s used: <a href="https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/#Startup">https://stefanescueduard.github.io/2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/#Startup</a>.</p><p>The <code>SigningIssuerCertificate</code> is used to get the <code>IssuerCertificate</code> or the public key; I will return to this class in a moment. The code below contains only what is necessary to use the public key in the Authentication service.</p><script src="https://gist.github.com/StefanescuEduard/4671d82a5b710017313b45f0b7dbd0af.js"></script><p>After the <code>Authentication</code> service was added, in the <code>Configure</code> method the <code>Authorization</code> and <code>Authentication</code> middleware needs to be added to the pipeline.</p><script src="https://gist.github.com/StefanescuEduard/3b7f8d14b342b24609d32d519976d391.js"></script><h2 id="SigningIssuerCertificate"><a href="#SigningIssuerCertificate" class="headerlink" title="SigningIssuerCertificate"></a><code>SigningIssuerCertificate</code></h2><p>In this class, the RSA class is used to create a <code>RsaSecurityKey</code> with the public key generated before.</p><script src="https://gist.github.com/StefanescuEduard/c766ed9e3e9ac416c8f483e088840f0d.js"></script><p><code>FromXmlString</code> initializes the <code>rsa</code> object with parameters from the XML files.<br>If we dig down in this method &ndash;&ndash; <a href="https://git.io/JvbVm">https://git.io/JvbVm</a> &ndash;&ndash; we can see that the <code>RSAParameters</code> are the same as they are in the XML file converted before.</p><p>The <code>rsa</code> is created on the constructor, this object must be disposed because there might be some resources that will run after the process ends.</p><script src="https://gist.github.com/StefanescuEduard/e104237af473b67124c0c6cf2cfe79c4.js"></script><h2 id="SigningAudience-Certificate"><a href="#SigningAudience-Certificate" class="headerlink" title="SigningAudience Certificate"></a><code>SigningAudience Certificate</code></h2><p><code>SigningAudienceCertificate</code> is very similar to the <code>SigningIssuerCertificate</code>, the only differences are that, is using the private key to initialize the <code>rsa</code> object and is returning <code>SigningCredentials</code> constructed with the <code>RsaSecurityKey</code> and the <code>SecurityAlgorithms</code>. For this, the <code>RsaSha256</code> algorithm is used because is the most recommended one. If you want to find what algorithm to use for each type of encryption, check out this article: <a href="https://auth0.com/blog/json-web-token-signing-algorithms-overview/">https://auth0.com/blog/json-web-token-signing-algorithms-overview/</a>.</p><script src="https://gist.github.com/StefanescuEduard/1dadcc525127a60f62e8b0b19f8abf46.js"></script><h2 id="AuthenticationService"><a href="#AuthenticationService" class="headerlink" title="AuthenticationService"></a><code>AuthenticationService</code></h2><p>This service is used by the <code>AuthenticationController</code> to authenticate the user. It is like a middleware because it’s using the <code>UserService</code> to validate the received <code>UserCredentials</code> and the <code>TokenService</code> to generate the JWT Token if the credentials were valid.</p><p>The <code>UserService</code> and <code>UserCredentials</code> were created in the previous article so I will use them from there. The <code>UserService</code> is a more likely a mock service, that has an internal list of users and checks if the given credentials are on that list. And the <code>UserCredentials</code> contains two properties <code>Username</code> and <code>Password</code>.</p><script src="https://gist.github.com/StefanescuEduard/fc8c8776b5b10c235d0ec90a03baddf3.js"></script><h2 id="TokenService"><a href="#TokenService" class="headerlink" title="TokenService"></a><code>TokenService</code></h2><p><code>TokenService</code> initializes on the constructor the <code>SigningAudienceCertificate</code> class created before. With this object, the <code>SigningCredentials</code> for the <code>TokenDescriptor</code> will be created.</p><script src="https://gist.github.com/StefanescuEduard/7ca742437c69dc1cfb04cc31587b74d2.js"></script><p>The <code>GetToken</code> method is used to generate the <code>TokenDescriptor</code> by using the <code>GetTokenDescriptor</code> method that will be explained in a moment; to create a <code>SecurityToken</code> from that descriptor and to get the token as a string from that object.</p><script src="https://gist.github.com/StefanescuEduard/915e50db78f2ca12eaa67063e760abf1.js"></script><p><code>GetTokenDescriptor</code> method creates a token with the minimum required properties: <code>Expires</code> and <code>SigningCredentials</code>. Also, the <code>Expires</code> property here is used because on the <code>Authentication</code> method the <code>LifetimeValidator</code> was set, but it doesn’t need to be specified.<br>All <code>SecurityTokenDescriptor</code> properties can be found on the Microsoft website: <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.identitymodel.tokens.securitytokendescriptor">https://docs.microsoft.com/en-us/dotnet/api/microsoft.identitymodel.tokens.securitytokendescriptor</a>.<br>The <code>GetAudienceSigningKey</code> method created before is used to generate the Token <code>SigningCredentials</code>, to validate that the Token was signed with the same private key from which the public key was generated.</p><script src="https://gist.github.com/StefanescuEduard/207fc69fd317387a32581f83506fc04b.js"></script><h2 id="AuthenticationController"><a href="#AuthenticationController" class="headerlink" title="AuthenticationController"></a><code>AuthenticationController</code></h2><p>In the <code>AuthenticationController</code> an endpoint is created to authenticate the user with <code>UserCredentials</code> and get the JWT Token by using the <code>AuthenticationService</code> described earlier.</p><script src="https://gist.github.com/StefanescuEduard/6c177445d14a723ac78abf737a1c2b80.js"></script><h2 id="ValidationController"><a href="#ValidationController" class="headerlink" title="ValidationController"></a><code>ValidationController</code></h2><p>And the <code>ValidationController</code> contains a plain endpoint that it’s using the <code>Authorize</code> attribute to validate the Token. Note that the Authentication Scheme must be used on the Authorize attribute and for the Authentication service.</p><script src="https://gist.github.com/StefanescuEduard/800e5b2315d5086c47b58dc3bb74a7dc.js"></script><h2 id="The-result"><a href="#The-result" class="headerlink" title="The result"></a>The result</h2><h3 id="Authentication"><a href="#Authentication" class="headerlink" title="Authentication"></a>Authentication</h3><p>Firstly the Authentication happy flow will be tested, so the combination of the username and password will match and the endpoint should provide the generated Token.</p>    <div>      <img src="authorized-authentication.png" alt="" data-action="zoom" class="photozoom">          </div><p>And secondly let’s test the unauthorized flow, where the provided credentials are wrong.</p>    <div>      <img src="unauthorized-authentication.png" alt="" data-action="zoom" class="photozoom">          </div><h3 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h3><p>Before checking that the Token is valid using the <code>ValidationController</code>, Auth0 crafted <a href="https://jwt.io/">https://jwt.io/</a> that decode the Token and check whether or not the Token is valid.</p>    <div>      <img src="jwt-token-validation.png" alt="" data-action="zoom" class="photozoom">          </div><p>On the Verify Signature section, both keys must be entered to validate the signature of the certificate.<br>Now the <code>ValidationController</code> will be used to check whether the token is valid or not, but this will happen internally, on the Authorize attribute. Firstly, the happy flow.</p>    <div>      <img src="valid-token.png" alt="" data-action="zoom" class="photozoom">          </div><p>And in the second test, the wrong token is validated.</p>    <div>      <img src="invalid-token.png" alt="" data-action="zoom" class="photozoom">          </div><hr><p>The source code from this article can be found on my GitHub account: <a href="https://github.com/StefanescuEduard/JwtAuthentication">https://github.com/StefanescuEduard/JwtAuthentication</a>.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> authentication </tag>
            
            <tag> asp </tag>
            
            <tag> asymmetric </tag>
            
            <tag> encryption </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JWT Authentication with Symmetric Encryption in ASP.NET Core</title>
      <link href="2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/"/>
      <url>2020/04/11/jwt-authentication-with-symmetric-encryption-in-asp-dotnet-core/</url>
      
        <content type="html"><![CDATA[<p>JWT Authentication is becoming one of the most used authentication types in modern web applications or services. This article covers the JWT Authentication with a Symmetric Key in ASP.NET Core. In the first part, there will be a short introduction into what Symmetric Key represents and the second part contains the prerequisites for this project and the actual implementation of this authentication type.<br>This article is the first article from of series of two, the second one will contain the authentication with an Asymmetric Key using a certificate.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>JWT Token is a common way of creating access tokens that can contain several claims (e.g. Username, Roles). JWT Token means JSON (JavaScript Object Notation) Web Token. Every JWT Token has the following structure:</p><ul><li>Header, containing the encryption algorithm;</li><li>Payload, containing custom Claims, plus at least two required claims:<ul><li><code>exp</code> representing the expiration time when the Token will become unavailable;</li><li><code>iat</code> or Issued at Time, the time when the Token was created;<br>The times are formatted using the Unix Timestamp format (e.g. 1582784721).</li></ul></li><li>Signature, representing the encoded header, plus <code>a dot</code>, plus the encoded payload, plus a secret key. The concatenated result will be run through the encryption algorithm specified on the Header to validate the Token.</li></ul><p>If you want to read more about JWT Token, this comprehensive paper covers all the concepts: <a href="https://tools.ietf.org/html/rfc7519">https://tools.ietf.org/html/rfc7519</a>.</p><h3 id="Symmetric-Key"><a href="#Symmetric-Key" class="headerlink" title="Symmetric Key"></a>Symmetric Key</h3><p>The Symmetric Key is used both for signing and validation. For example, let’s say the John wants to share a secret with Jane, when the secret is told, John also tells Jane a password - the key - in order for the secret to be understood. In this way, John - the identity provider or the service - ensures that his secret is well kept by using the given password.</p>    <div>      <img src="symmetric-key.svg" alt="Symmetric Key" data-action="zoom" class="photozoom">          </div><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p>ASP.NET Core 3.1 will be used for this project. Microsoft also offers a great package that provides all that is needed to create a JWT Token-based authentication. The package is called <code>Microsoft.AspNetCore.Authentication.JwtBearer</code>, this is the only package that the project needs, and can be found here: <a href="https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer">https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer</a>.</p><h2 id="Creating-a-secret-key"><a href="#Creating-a-secret-key" class="headerlink" title="Creating a secret key"></a>Creating a secret key</h2><p>The signing and validation key will be a user secret key. ASP.NET provides the user secret key feature to store all the confidential data that doesn’t have to be committed or shared outside the user or developer environment. For the production or testing environments, the keys need to be store in a cloud vault, like Microsoft Azure offers through Key Vault - <a href="https://azure.microsoft.com/en-us/services/key-vault/">https://azure.microsoft.com/en-us/services/key-vault/</a> -, but this will be a topic for another article.<br>Firstly, the project needs to be initiated for using user secrets, by running the following command in the project folder:</p><script src="https://gist.github.com/StefanescuEduard/95c685ba92b5068bd7fbfe970e32c0e4.js"></script><p>Then the user secret key is added, using the following command:</p><script src="https://gist.github.com/StefanescuEduard/de3edb36cecfc650b434206fd7c5eda4.js"></script><p>This command will add to the <code>secrets.json</code> file the <code>AppSettings:EncryptionKey</code> key with the value <code>POWERFULENCRYPTIONKEY</code>.<br>If there are multiple values for the <code>AppSettings</code> then this key can become more readable by using a JSON format like:</p><script src="https://gist.github.com/StefanescuEduard/809bec7d49bc807774bd7fc646a5f8c4.js"></script><p>The <code>POWERFULENCRYPTIONKEY</code> will be encoded in an array of bytes and then this binary will be Base64 encoded, this is required for both signing and validation.</p><h2 id="Startup"><a href="#Startup" class="headerlink" title="Startup"></a>Startup</h2><p>In the <code>ConfigureServices</code> method from the <code>Startup</code> class, the <code>AppSettings</code> section needs to be read. To read a type from the configuration file, a class must be created, so for the <code>AppSettings</code> section an equivalent class needs to exists, as is shown below. This class can be seen as a Data Transfer Object, which contains plain properties.</p><script src="https://gist.github.com/StefanescuEduard/4fc78f767647bb27f6dfacf2534f34a7.js"></script><p>After the section is read, the <code>EncryptionKey</code> needs to be converted into bytes.</p><script src="https://gist.github.com/StefanescuEduard/84b4039f2f5ff38bb154003eb093867c.js"></script><p>On <code>line 9</code> the <code>Authentication</code> service is added into the App container, this service is responsible, with the managing <code>Authentication</code> settings, like <code>IssuerSigningKey</code> or <code>LifeTimeValidation</code>.<br>For this step, an extension method is created called <code>AddAuthentication</code>, which receives the <code>service</code> and the <code>signingKey</code> converted earlier.<br>From <code>line 11</code> to <code>14</code>, the services are configured for the Dependency Injection, we will return to the implementation of these services in a moment.</p><p>Let’s return to the <code>AddAuthentication</code> method:</p><script src="https://gist.github.com/StefanescuEduard/001c342d1f1b6a52faf2aa0709cd1ecb.js"></script><p>The <code>authenticationOptions</code> need to configure the <code>Authenticate</code> and <code>Challenge</code> Schemes, in order to verify that the endpoint(s) which receives a JWT Token will go through the validation step, as is described below starting from <code>line 12</code>. The same Schema will be seen on the endpoints that use the <code>AuthorizeAttribute</code>.<br>Then the <code>JwtBearer</code> is added to the <code>Authentication</code> process, using the following properties:</p><ul><li><code>SaveToken</code> is self-explanatory. It’s used to persist the Token into a local storage. The token will be valid even if the service restarts, so its lifetime is different from the application;</li><li><code>ValidateAudience</code> and <code>ValidateIssuer</code> must be used for the service to skip or to validate the Audience or the Issuer. The Audience refers to the server or the Identity Provider, in this case our ASP.NET Service. And the <code>Issuer</code> refers to the client(s) that makes HTTP request(s). For the sake of this example, both are set <code>false</code>. Please note that even if you don’t want to validate the <code>Audience</code> or/and the <code>Issuer</code> these values must be set;</li><li><code>ValidateIssuerSigningKey</code> needs to be set to <code>true</code>, in order to validate the received Token;</li><li>For <code>IssuerSigningKey</code> will use the <code>SymmetricSecurityKey</code>, the same approach will be also used when the Token will be created.</li><li><code>LifeTimeValidator</code> is important if the generated <code>Token</code> has set an expiration time.</li></ul><p>All the JWT Bearer Options can be found on the Microsoft website: <a href="https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.authentication.jwtbearer.jwtbeareroptions">https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.authentication.jwtbearer.jwtbeareroptions</a>.</p><p>The <code>LifeTimeValidator</code> handler is checking if the expiration date is greater than the current Date, as follows:</p><script src="https://gist.github.com/StefanescuEduard/9179957eaf0cc9765c02ccf9dc391637.js"></script><p>After the services were configured, the <code>Authentication</code> and <code>Authorization</code> middleware must be added to the App pipeline in the <code>Configure</code> method.</p><script src="https://gist.github.com/StefanescuEduard/3b7f8d14b342b24609d32d519976d391.js"></script><h2 id="UserCredentials"><a href="#UserCredentials" class="headerlink" title="UserCredentials"></a><code>UserCredentials</code></h2><p>User’s Credentials will be used as a Data Transfer Object, this class will be received on the authentication endpoint and sent to the <code>AuthenticationService</code>. It’s a plain class that contains only the <code>Username</code> and the <code>Password</code> of the user.</p><script src="https://gist.github.com/StefanescuEduard/221e8de96fe412ebf143346d02df389b.js"></script><h2 id="AuthenticationService"><a href="#AuthenticationService" class="headerlink" title="AuthenticationService"></a><code>AuthenticationService</code></h2><p>The <code>AuthenticationService</code> is used like a middleware which receives the <code>UserCredentials</code> from the <code>Controller</code>, validate them using the <code>UserService</code> and if the credentials are valid, it creates a Token using the <code>TokenService</code>. Both the <code>User</code> and <code>Token</code> services are injected on the constructor.</p><script src="https://gist.github.com/StefanescuEduard/fc8c8776b5b10c235d0ec90a03baddf3.js"></script><h2 id="UserService"><a href="#UserService" class="headerlink" title="UserService"></a><code>UserService</code></h2><p>For the sake of this example, the <code>UserService</code> contains a list of users created on the constructor. In a real-life scenario, this will be read from a storage or from a service.</p><script src="https://gist.github.com/StefanescuEduard/42b61657f6c727d39d2a988a88613b6f.js"></script><p>This is more like a <code>UserValidation</code> service, but to better illustrate that it also reads the users, the <code>UserService</code> name will be kept.</p><script src="https://gist.github.com/StefanescuEduard/75dd9668eab79fd03e2a0450819cef5b.js"></script><p>The <code>ValidateCredentials</code> method checks if the <code>username</code> and <code>password</code> pair exists, and if it doesn’t it will throw the <code>InvalidCredentialsException</code> which will be caught on the <code>Controller</code>.</p><h2 id="TokenService"><a href="#TokenService" class="headerlink" title="TokenService"></a><code>TokenService</code></h2><p><code>TokenService</code> is receiving on the constructor the <code>AppSettings</code>, which will be used on the <code>GetTokenDescriptor</code> method to set up the Token.</p><script src="https://gist.github.com/StefanescuEduard/f60ae2f42b5149c9c0042e8cc472f9e7.js"></script><p>The public <code>GetToken</code> method is used to get the token description, to create the Token and write it into a string, that will be returned to the calling service, in this case to the <code>AuthenticationService</code>.</p><script src="https://gist.github.com/StefanescuEduard/ec1cb7b8308868d491cd9d078715c253.js"></script><p>On the <code>GetTokenDescriptor</code> method, the token is constructed. In this method, the <code>ExpirationTime</code> and <code>SigningCredentials</code> are set. Because the Claims are not in the main focus of this article, I will create another one, in which I will explain how the Claims can be set on the Token and how they can be used.</p><script src="https://gist.github.com/StefanescuEduard/f38f48801ddaf2af33ce4804840f4de8.js"></script><p>All the Token Descriptors can be found on the Microsoft website: <a href="https://docs.microsoft.com/en-us/dotnet/api/system.identitymodel.tokens.securitytokendescriptor">https://docs.microsoft.com/en-us/dotnet/api/system.identitymodel.tokens.securitytokendescriptor</a>.</p><h2 id="AuthenticationController"><a href="#AuthenticationController" class="headerlink" title="AuthenticationController"></a><code>AuthenticationController</code></h2><p>Now, all we have to do, is to create an <code>AuthenticationController</code> which receives the <code>UserCredentials</code> and uses the previously created <code>AuthenticationService</code>.<br>On the constructor the <code>AuthenticationService</code> is injected, to be used on the <code>Authentication</code> endpoint.</p><script src="https://gist.github.com/StefanescuEduard/c98a7bc5ad4142c939ad54334d8d8955.js"></script><p>The authentication endpoint accepts HTTP Post requests, receives the <code>UserCredentials</code> as previously mentioned and uses the <code>AuthenticationService</code> to authenticate and create the Token.</p><script src="https://gist.github.com/StefanescuEduard/2330ad06d6bd4c693095f76a29f81171.js"></script><p>If the credentials are valid, then the endpoint will return an <code>OK</code> HTTP Status code and the generated token. Otherwise, if the <code>InvalidCredentialsException</code> is thrown, the <code>Unauthorized</code> HTTP Status code is returned.</p><h2 id="ValidationController"><a href="#ValidationController" class="headerlink" title="ValidationController"></a><code>ValidationController</code></h2><p>The purpose of the <code>ValidationController</code> is to check that the signing process is working, in order to validate the Token.</p><script src="https://gist.github.com/StefanescuEduard/0469aa49ff0516acbd0d3c4ef8afec82.js"></script><p>You may notice that the <code>Validate</code> endpoint has the <code>AuthorizeAttribute</code> which has on its constructor the same <code>AuthenticationSchemes</code> as was set on the <code>Authentication</code> service.</p><h2 id="The-result"><a href="#The-result" class="headerlink" title="The result"></a>The result</h2><p>Firstly, the happy flow for the <code>AuthenticationController</code> is tested, so we’ll provide the correct combination of the username and password, in order to receive the token.</p>    <div>      <img src="happy-flow-authentication.png" alt="Happy flow authentication" data-action="zoom" class="photozoom">          </div><p>Let’s test with credentials that are not correct, the response should be Unauthorized.</p>    <div>      <img src="unauthorized-authentication.png" alt="Unauthorized authentication" data-action="zoom" class="photozoom">          </div><p>And secondly, the generated token needs to be tested using the <code>Validation</code> controller. The first test will be with the generated token, to see that the validation is passed.</p>    <div>      <img src="happy-flow-validation.png" alt="Happy flow validation" data-action="zoom" class="photozoom">          </div><p>And the second test is when the wrong token is provided for validation.</p>    <div>      <img src="unauthorized-validation.png" alt="Unauthorized flow validation" data-action="zoom" class="photozoom">          </div><hr><p>The source code from this article can be found on my GitHub account: <a href="https://github.com/StefanescuEduard/JwtAuthentication">https://github.com/StefanescuEduard/JwtAuthentication</a>.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> authentication </tag>
            
            <tag> asp </tag>
            
            <tag> encryption </tag>
            
            <tag> symmetric </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ Consumer Received Event with Docker in .NET</title>
      <link href="2020/04/04/rabbitmq-consumer-events-with-docker-in-dotnet/"/>
      <url>2020/04/04/rabbitmq-consumer-events-with-docker-in-dotnet/</url>
      
        <content type="html"><![CDATA[<p>This article contains another approach of consuming messages. The first part will be a comparison between the several types of consuming messages and in the second part, each line of code will be explained.<br>The entire environment setup with Docker can be found in the first article from the RabbitMQ series. In this series, there are also explained some core principles about each RabbitMQ node. I highly recommend that, if you are at the beginning with RabbitMQ or with the AMQP standard, to start with these articles:</p><ol><li><a href="https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/</a></li><li><a href="https://stefanescueduard.github.io/2020/03/07/rabbitmq-exchange-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/03/07/rabbitmq-exchange-with-docker-in-dotnet/</a></li><li><a href="https://stefanescueduard.github.io/2020/03/14/rabbitmq-queue-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/03/14/rabbitmq-queue-with-docker-in-dotnet/</a></li><li><a href="https://stefanescueduard.github.io/2020/03/21/rabbitmq-consumer-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/03/21/rabbitmq-consumer-with-docker-in-dotnet/</a></li></ol><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The RabbitMQ Client provides a <code>Received</code> event that will be used to consume the messages coming from one or more <code>Queues</code>. In the <code>Consumer</code> article, the messages are consumed and acknowledged using the <code>BasicGet</code> and the <code>BackAck</code> methods in a while loop, waiting for the <code>CancellationToken</code>. The advantage of that method is that a message can be consumed when needed. So, care must be taken when using this method. It’s not recommended to use it in an infinite while loop if the <code>Received</code> event exists, and it’s a good replacement for that type of approach.<br>The advantage of the <code>Received</code> event is obvious, that will no longer consume unnecessary resources, but the “disadvantage” is that the messages will be consumed whenever they are raised based on the <code>routing key</code>.<br>But this depends on the different scenarios which way to choose. A useful tip that may help when you must choose between these two ways of consuming messages is:</p><ul><li><code>BasicGet</code> method can be used when there are more messages to get or it needs to be consumed at a certain time;</li><li><code>Received</code> event can be used when the raised message needs to be consumed immediately;</li></ul><p>There is also another approach to consume messages, by inheriting the <code>DefaultBasicConsumer</code> class. This gives the advantage of having a hierarchy of consumers, which may lead to different design patterns.</p><p>In the following part of this article, there will be only some chunks of code, which are required to consume the messages using the <code>Received</code> event. The connection setup is the same as in previous articles.</p><h2 id="Received-event"><a href="#Received-event" class="headerlink" title="Received event"></a><code>Received</code> event</h2><p>Before start establishing the connection, a <code>CancellationToken</code> is needed to wait for the user to stop the Console App execution.</p><p>To bind the <code>Consumer</code> to the <code>Channel</code>, a new <code>EventingBasicConsumer</code> instance is made by passing the <code>Channel</code> as parameter.</p><script src="https://gist.github.com/StefanescuEduard/e2c8b47c6acdbd68d1b1250053417f76.js"></script><p>An important note here is that there is no need to surround the <code>IConnection</code> and the <code>IChannel</code> into a using statement, the reason for this is that the <code>Connection</code> and the <code>Channel</code> need to exist as long as the app. If these two variables were disposed too early, then no message would be received.<br>You may notice that in this RabbitMQ series, I used the <code>Channel</code> word instead of the official name <code>Model</code>, that’s because it’s easier to understand. For me, a <code>Model</code> is too generic, instead, a <code>Channel</code> makes me think to a communication channel, which in fact it really is.<br>The official documentation for <code>EventingBasicConsumer</code> can be found here: <a href="https://www.rabbitmq.com/releases/rabbitmq-dotnet-client/v3.1.1/rabbitmq-dotnet-client-3.1.1-client-htmldoc/html/type-RabbitMQ.Client.Events.EventingBasicConsumer.html">https://www.rabbitmq.com/releases/rabbitmq-dotnet-client/v3.1.1/rabbitmq-dotnet-client-3.1.1-client-htmldoc/html/type-RabbitMQ.Client.Events.EventingBasicConsumer.html</a>.</p><h3 id="Subscribing-to-the-event"><a href="#Subscribing-to-the-event" class="headerlink" title="Subscribing to the event"></a>Subscribing to the event</h3><p>Using the <code>Consumer</code> created earlier, we subscribe to the <code>Received</code> event the <code>OnNewMessageReceived</code> handler.</p><script src="https://gist.github.com/StefanescuEduard/f52c65f4e85caab49614b6072a94b25c.js"></script><h3 id="Handling-the-event"><a href="#Handling-the-event" class="headerlink" title="Handling the event"></a>Handling the event</h3><p>This event handler will display the received message and another message to inform the user that can stop consuming messages.</p><script src="https://gist.github.com/StefanescuEduard/7bedd0d607840b0b7f25a5bc92413e46.js"></script><h3 id="Consuming-messages"><a href="#Consuming-messages" class="headerlink" title="Consuming messages"></a>Consuming messages</h3><p>To start consuming messages, the <code>Consumer</code> will be bind to the queue. In the following code, there is an iteration through the queues count entered previously. Then the user is asked to enter the <code>Queue</code> name that will be bound to the <code>Consumer</code>.<br>The <code>BasicConsume</code> method takes three parameters:</p><ul><li>the first one is the <code>Queue</code> name;</li><li>the second is used for auto acknowledgment, in this case, is set to <code>true</code>, but if it was set to <code>false</code> then the acknowledgment had to be done manually if this is wanted;</li><li>and the third one is the <code>Consumer</code>;</li></ul><h3 id="Waiting-for-Cancellation"><a href="#Waiting-for-Cancellation" class="headerlink" title="Waiting for Cancellation"></a>Waiting for <code>Cancellation</code></h3><p>The <code>WaitHandle</code> static class is a handy way to wait for the <code>CancellationToken</code> to be raised. Instead of the <code>CancellationToken</code>, the <code>AutoResetEvent</code> or other signaling events can be also used.</p><script src="https://gist.github.com/StefanescuEduard/1fdd2c18bee80962677a720eddbce303.js"></script><h3 id="Disposing-resources"><a href="#Disposing-resources" class="headerlink" title="Disposing resources"></a>Disposing resources</h3><p>After the user indicates that no longer wants to receive messages, or in a real-life scenario, when the application ends, all the created resources need to be disposed.</p><script src="https://gist.github.com/StefanescuEduard/be8d01bb086ce8331d780cfb5ca6a499.js"></script><p>Firstly the <code>OnNewMessageReceived</code> event handler needs to be unsubscribed from the <code>Received</code> to stop displaying messages after the program finishes its execution. It can be possible that reference to event handler can still exist after the resources are disposed and the program closes. So, I highly recommend that all the time when it’s needed, to unsubscribe the event handlers.<br>After that, the <code>Channel</code> and the <code>Connection</code> are closed and disposed.</p><h3 id="The-result"><a href="#The-result" class="headerlink" title="The result"></a>The result</h3><p>Using the <code>Producer</code>, the direct <code>Exchange</code> and one <code>Queue</code> created in the previous articles, the topology will be created and the <code>Consumer</code> will be connected to it.<br>In the picture below, the sent message by the <code>Producer</code> using a specific <code>routing key</code> bound to the <code>Queue</code> reached the event handler, and this displayed the message.</p>    <div>      <img src="consumer-received-message.png" alt="Consumer received message" data-action="zoom" class="photozoom">          </div><hr><p>All the code for this <code>Consumer</code> and for the entire topology is available on my GitHub account: <a href="https://github.com/StefanescuEduard/RabbitMQ_POC">https://github.com/StefanescuEduard/RabbitMQ_POC</a></p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> amqp </tag>
            
            <tag> consumer </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ Headers Exchange with Docker in .NET</title>
      <link href="2020/03/28/rabbitmq-headers-exchange-with-docker-in-dotnet/"/>
      <url>2020/03/28/rabbitmq-headers-exchange-with-docker-in-dotnet/</url>
      
        <content type="html"><![CDATA[<p>This article is about <code>Headers Exchange</code>, I chose to create an article just for this type of <code>Exchange</code> because it needs a little more attention compared to the other three types (i.e. fanout, direct, topic).<br>In this article, I will use the setup environment with Docker from the first article, so if you want a good starting point, you can begin with the first article: <a href="https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/</a>.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><code>Headers Exchange</code> is routing the messages based on the bindings that are applied to the <code>Producer</code> and <code>Queue</code>.<br>In order to produce a message, that message should be published with defined properties that are also bound to the <code>Queue</code> - this is why it’s called <code>Headers Exchange</code> - and can also be seen as the <code>routing key</code>.<br>Another mandatory property that must be bound to the <code>Queue</code> is <code>x-match</code>. This property specifies the matching criteria, as follows:</p><ul><li><code>x-match=all</code> means that all the header pairs must match;</li><li><code>x-match=any</code> means that at least one header pairs must match;</li></ul><p>A quick example, that will be also implemented using .NET Core:<br>Let’s say that a Windows Application produces two logging types, information and error, and the server process these log messages based on its types.<br>So for each log type, will be a <code>Queue</code> that will have a property <code>log-level</code> equals to the type of logging that will receive. To send log information, the <code>Producer</code> must set to that message a property <code>log-level=information</code> so that the <code>Exchange</code> will forward the message to the correct <code>Queue</code>.</p><p>If this is still unclear, then a brief explanation for <code>Headers Exchange</code> is that the messages contain a <strong>header</strong> in order to be bound correctly.<br>I hope now the whole typology is a little bit clearer, and why this type of <code>Exchange</code> is called <code>Headers</code>.</p><p>Now we can start implementing these concepts, the following part will contain code chunks that are required in order to use <code>Headers Exchange</code>, these chunks were applied to existing code from the previous four articles about RabbitMQ.</p><h2 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a><code>Producer</code></h2><p>To bind the logging type to the <code>Producer</code> properties, a <code>Dictionary</code> of type <code>string, object</code> is used as the properties headers. In this case, the <code>key</code> will be <code>log-level</code> and the <code>value</code> will be the actual log level (i.e. information or error). For this scenario, the log level is entered by the user, but in a real-life scenario, there will be a logging system that will serve this scope.</p><script src="https://gist.github.com/StefanescuEduard/0db265cae4058c75ca8a1051f31b605c.js"></script><p>The <code>propertiesHeaders</code> are bound to the published message by firstly creating plain properties with an empty content header using the <code>CreateBasicProperties</code> method. The difference between this <code>Producer</code> and the one from the first article is on line 14, which sets the <code>Headers</code> property to the dictionary created earlier.</p><h2 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a><code>Exchange</code></h2><p>This type of <code>Exchange</code> is very similar to the one created in the second article. The only thing that changes is the type, which will be set to <code>headers</code> when the <code>Exchange</code> is declared on line 7.</p><script src="https://gist.github.com/StefanescuEduard/613861853dea188e25884ddedec856ac.js"></script><p>We can see this result also on the RabbitMQ Management page:</p>    <div>      <img src="rabbitmq-management-exchange.png" alt="RabbitMQ Management Exchange" data-action="zoom" class="photozoom">          </div><h2 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a><code>Queue</code></h2><p>To bound the <code>Queue</code>, the properties of the header must be set using a <code>Dictionary</code> of type <code>string, string</code> that will be passed as arguments to the <code>QueueBind</code> method.<br>For this article, I chose to use the <code>all</code> value for the first <code>x-match</code> property, but this value can be also set to <code>any</code> because only the <code>log-level</code> header needs to match.<br>And the second property is the <code>log-level</code>, the value of this property is given by the user, but in a real-life scenario, this property will be set by the listener service.</p><script src="https://gist.github.com/StefanescuEduard/c897970461e039d91b0f85ec352823a1.js"></script><p>When a <code>Queue</code> is created the result can be also seen on the RabbitMQ Management page. In the picture below, there is an <code>information-queue</code> bound with <code>log-level</code> set to <code>information</code> using the code above.</p>    <div>      <img src="rabbitmq-management-queue.png" alt="RabbitMQ Management Queue" data-action="zoom" class="photozoom">          </div><h2 id="The-result"><a href="#The-result" class="headerlink" title="The result"></a>The result</h2><p>With all of this in place, we can start using this topology.</p><p>Firstly, we have to create a <code>Headers Exchange</code>:</p>    <div>      <img src="headers-exchange-created-successfully.png" alt="Headers Exchange created successfully" data-action="zoom" class="photozoom">          </div><p>The <code>information</code> and <code>error</code> queues have to be created and bound the <code>log-level</code> property to them:</p>    <div>      <img src="queues-created-successfully.png" alt="Queues created successfully" data-action="zoom" class="photozoom">          </div><p>The <code>Consumer</code> from the last article will be used, the one without events, but the one with events can be used as well. There is no need to create another <code>Consumer</code> because its responsibility is to listen to <code>Queues</code> that are subscribed to.</p><p>Now the messages can be produced. There will be two messages for the <code>information</code> and <code>error</code> log levels, and one that is not bound to any header.</p>    <div>      <img src="producing-consuming-messages.png" alt="Producing and consuming messages" data-action="zoom" class="photozoom">          </div><p>In the picture above can be seen, that the first two messages which had the headers bound correctly were received by the consumer, but the final messages which don’t have any headers were not received.</p><p>The entire code for this topology and the RabbitMQ series can be found on my GitHub account: <a href="https://github.com/StefanescuEduard/RabbitMQ_POC">https://github.com/StefanescuEduard/RabbitMQ_POC</a>.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> amqp </tag>
            
            <tag> docker </tag>
            
            <tag> exchange </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ Consumer with Docker in .NET</title>
      <link href="2020/03/21/rabbitmq-consumer-with-docker-in-dotnet/"/>
      <url>2020/03/21/rabbitmq-consumer-with-docker-in-dotnet/</url>
      
        <content type="html"><![CDATA[<p>This is the last article from the RabbitMQ series. In this series, I explained all the RabbitMQ nodes using .NET and Docker. There will be other two articles, the first one is dedicated just for the <code>Headers Exchange</code>, because this type of <code>Exchange</code> needs more attention, considering that the sent messages need to be binding to the <code>Exchange</code> using <code>x-match</code> property and to the <code>Queue</code> using defined properties. And in the second one, I will explain a safer way of consuming messages and closing the connection, because this series is more educational oriented and plain methods were used for a better understanding.</p><p>In this article the <code>Consumer</code> node of the RabbitMQ topology will be presented, in the first part the core concepts will be cover, and in the second part, each line of code will be explained.<br>The first article from this series contains the environment setup with Docker and core fundamentals of RabbitMQ. You can check the other three articles here:</p><ol><li><a href="https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/</a> (environment setup)</li><li><a href="https://stefanescueduard.github.io/2020/03/07/rabbitmq-exchange-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/03/07/rabbitmq-exchange-with-docker-in-dotnet/</a></li><li><a href="https://stefanescueduard.github.io/2020/03/14/rabbitmq-queue-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/03/14/rabbitmq-queue-with-docker-in-dotnet/</a></li></ol><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The RabbitMQ <code>Consumer</code> it’s the node to which the <code>Queues</code> are connected. This will receive all the messages sent by the <code>Producer</code> and followed some path to be consumed. A real-life example for the <code>Consumer</code> it will be, a logging system, where the <code>Producer</code> is the app that sent the log message and the <code>Consumer</code> is the Console or the API that will process the received message.</p><h3 id="Useful-tips"><a href="#Useful-tips" class="headerlink" title="Useful tips"></a>Useful tips</h3><p>Here are some tips that I found useful:</p><ul><li>The lifetime of a <code>Consumer</code> is as long as the application lifetime;</li><li>It’s not recommended to have a <code>Consumer</code> that consume only one message, but it’s totally fine to have a <code>Consumer</code> subscribed to a <code>Queue</code> as long as it sent multiple messages. That’s because, for obvious reasons, it’s not necessary to have this entire system to send only one message;</li><li>A <code>Consumer</code> has a uniquely identifiable tag and a subscription ID, which is the connection link with the <code>Queue</code>;</li><li>The exclusivity to the <code>Consumer</code> can be made using the <code>exclusive</code> flag, that means that the <code>Consumer</code> will receive messages from only one <code>Queue</code>;</li><li>And some <code>Consumers</code> can have a higher priority to the same messages over other <code>Consumers</code>;</li></ul><p>All the <code>Consumers</code>‘s properties can be read on the RabbitMQ website: <a href="https://www.rabbitmq.com/consumers.html">https://www.rabbitmq.com/consumers.html</a>.</p><h2 id="Creating-the-Consumer"><a href="#Creating-the-Consumer" class="headerlink" title="Creating the Consumer"></a>Creating the <code>Consumer</code></h2><p>First of all, we need to know how many queues are listening to this consumer, so a message is prompt to enter the number of queues. After that, a connection to the RabbitMQ Server is made using a URI and specifying the <code>ConnectionTimeout</code> to its maximum value. This value is used just for this article purposes, also all the <code>ConnectionFactory</code> properties with few examples can be found here: <a href="https://www.rabbitmq.com/releases/rabbitmq-dotnet-client/v3.2.4/rabbitmq-dotnet-client-3.2.4-client-htmldoc/html/type-RabbitMQ.Client.ConnectionFactory.html">https://www.rabbitmq.com/releases/rabbitmq-dotnet-client/v3.2.4/rabbitmq-dotnet-client-3.2.4-client-htmldoc/html/type-RabbitMQ.Client.ConnectionFactory.html</a>.<br>You can check the first article where is another approach of creating the <code>ConnectionFactory</code> using the properties <code>Hostname</code>, <code>UserName</code> and <code>Password</code>.</p><script src="https://gist.github.com/StefanescuEduard/2ec14bb22789a00f6bd2364f60767cda.js"></script><p>You may notice that the <code>IConnection</code> and <code>IModel</code> fields are static, that’s because this is a Console App, and these two fields are used inside of each thread that is listening to a <code>Queue</code>. But these fields will be closed and disposed before the application finishes its execution.</p><h3 id="Listening-to-the-Queue-s"><a href="#Listening-to-the-Queue-s" class="headerlink" title="Listening to the Queue(s)"></a>Listening to the <code>Queue(s)</code></h3><p>Before start listening to the <code>Queue(s)</code>, a <code>CancellationTokenSource</code> is created in order to be passed on each execution thread that will display a message, that’s because the thread will have the same lifetime as the application, and it’s also a safety check to cancel the <code>Consumer</code> subscription once the application ends.<br>According to the number of <code>Queues</code> entered, this <code>Consumer</code> is listening to, the <code>Queue</code> name is asked in order to create the connection between the <code>Consumer</code> and the <code>Queue</code>.<br>Then for each <code>Queue</code>, a thread is created and start listening to it.</p><script src="https://gist.github.com/StefanescuEduard/abe0560836b659b37edf04f26ee70354.js"></script><h3 id="Displaying-the-received-message"><a href="#Displaying-the-received-message" class="headerlink" title="Displaying the received message"></a>Displaying the received message</h3><p>The <code>DisplayQueueMessage</code> has the following parameters:</p><ul><li>The <code>queueName</code> which will listen to;</li><li>And the <code>cancellationToken</code> which represents the listener thread lifetime;<script src="https://gist.github.com/StefanescuEduard/c84eb5e70cd2ae7dc41a0f16fd5aac1c.js"></script></li></ul><h4 id="DisplayQueueMessage-method-explanation"><a href="#DisplayQueueMessage-method-explanation" class="headerlink" title="DisplayQueueMessage method explanation"></a><code>DisplayQueueMessage</code> method explanation</h4><p>As is described earlier, the listener will exist until the <code>cancellationToken</code> is not requested.<br>On line 5 the message is retrieved from the <code>Queue</code> and is checked if it’s <code>null</code>. If it’s not <code>null</code>, the message body which is an array of bytes will be decoded using the same encoding format as it was used to encode the message.<br>After displaying the retrieved message, the result is acknowledged in order to be deleted from the <code>Queue</code> and to display the next message.<br>This is not the nicest way to listen to the messages and can cause performance issues, that’s why I will create another article about listening events, that will be raised only when a message is published and redirected to a specific <code>Queue</code>.</p><h3 id="Ending-the-connection"><a href="#Ending-the-connection" class="headerlink" title="Ending the connection"></a>Ending the connection</h3><p>After the thread was created for each <code>Queue</code>, a message is prompt in order to display how many <code>Queues</code> are listening to the <code>Consumer</code>.</p><script src="https://gist.github.com/StefanescuEduard/bfe025f32b5cd5fabfaf7af89fecf6fc.js"></script><p>Then the <code>cancellationTokenSource</code> is cancelled, in order to stop the listening threads, after that the connection and channel are closed and disposed.<br>The <code>WaitHandle</code> is used to end the connection and the channel safely after the cancellation is signalled.</p><p>You can find the solution, with all four nodes on my Github account: <a href="https://github.com/StefanescuEduard/RabbitMQ_POC">https://github.com/StefanescuEduard/RabbitMQ_POC</a>.</p><h3 id="Topology-result"><a href="#Topology-result" class="headerlink" title="Topology result"></a>Topology result</h3><p>By following the <a href="http://tryrabbitmq.com/">http://tryrabbitmq.com/</a> topology described in the previous articles, firstly the <code>Exchange</code> is created:</p>    <div>      <img src="exchange-creation.png" alt="Exchange creation" data-action="zoom" class="photozoom">          </div><p>Then two <code>Queues</code> are created:</p>    <div>      <img src="queues-creation.png" alt="Queues creation" data-action="zoom" class="photozoom">          </div><p>The <code>Consumer</code> is connected to them:</p>    <div>      <img src="consumer-creation.png" alt="Consumer creation" data-action="zoom" class="photozoom">          </div><p>The last step is to publish the messages and checking the <code>Consumer</code> that is displaying them.</p><ul><li><p>Producing messages for both <code>routing-keys</code>:</p>    <div>      <img src="producer-creation.png" alt="Producer creation" data-action="zoom" class="photozoom">          </div></li><li><p>Checking <code>Consumer</code> to see the sent messages:</p>    <div>      <img src="consumer-listening.png" alt="Consumer listening" data-action="zoom" class="photozoom">          </div></li></ul><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> amqp </tag>
            
            <tag> consumer </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ Queue with Docker in .NET</title>
      <link href="2020/03/14/rabbitmq-queue-with-docker-in-dotnet/"/>
      <url>2020/03/14/rabbitmq-queue-with-docker-in-dotnet/</url>
      
        <content type="html"><![CDATA[<p>This is the third article from a series of four about RabbitMQ <code>Queue</code>. The first part contains a brief introduction into the <code>Queue</code> concepts and in the second part, each line of the code is explained. You can check the first article where the RabbitMQ core concepts are presented and also the environment setup with Docker and the <code>Producer</code> are explained step by step.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The RabbitMQ <code>Queue</code> is positioned in the <code>Message Broker</code> section with the <code>Exchange</code>, and can be seen as buffer storage, that receives messages through a stream based on the <code>binding key</code> from the <code>Exchange</code> which also receives the messages from one or many <code>Producers</code>. They are acting like a Queue Data Structure, that can be <code>enqueued</code> and <code>dequeued</code>, using the FIFO rule.<br>Some tips about the <code>Queues</code>, that I find useful are:</p><ul><li>A <code>Queue</code> name can’t start the <code>amq</code> name, because is restricted by the <code>Broker</code> for internal usages.</li><li>If the <code>Queue</code> name is not specified a random one will be assigned to it.</li><li><code>Queues</code> with the same name can be created on the same channel, by only the last one will be kept.</li></ul><p>All the RabbitMQ <code>Queue</code> properties can be on their site: <a href="https://www.rabbitmq.com/queues.html">https://www.rabbitmq.com/queues.html</a>.</p><h2 id="Creating-the-Queue"><a href="#Creating-the-Queue" class="headerlink" title="Creating the Queue"></a>Creating the <code>Queue</code></h2><p>To start creating a <code>Queue</code> firstly the connection to the RabbitMQ server should be built using the <code>ConnectionFactory</code>; is the same setup as in the previous article about RabbitMQ <code>Exchange</code> using a <code>URI</code>. But in the first article where the <code>Producer</code> is created, the connection was created using explicit properties of <code>ConnectionFactory</code> (i.e. <code>Hostname</code>, <code>UserName</code> and <code>Password</code>).</p><h3 id="Connection-setup"><a href="#Connection-setup" class="headerlink" title="Connection setup"></a>Connection setup</h3><p>As you can see in the below chunk code, all these properties are embedded into one <code>Uri</code> instance.</p><script src="https://gist.github.com/StefanescuEduard/9eba93a966e3f3367ad21ab023cc0bd4.js"></script><p><code>ConnectionTimeout</code> property was set to its maximum value because I didn’t want the connection to expire, but this is just for this article purposes. If you are interested to find all the <code>ConnectionFactory</code> properties, they are well documented here with one example: <a href="https://www.rabbitmq.com/releases/rabbitmq-dotnet-client/v3.2.4/rabbitmq-dotnet-client-3.2.4-client-htmldoc/html/type-RabbitMQ.Client.ConnectionFactory.html">https://www.rabbitmq.com/releases/rabbitmq-dotnet-client/v3.2.4/rabbitmq-dotnet-client-3.2.4-client-htmldoc/html/type-RabbitMQ.Client.ConnectionFactory.html</a>. This is the documentation for version 3.2.4 when you read this article the RabbitMQ Client for .NET may be updated, so it’s possible that some properties may be changed, deprecated or removed.</p><h3 id="Developer-input"><a href="#Developer-input" class="headerlink" title="Developer input"></a>Developer input</h3><p>You may wonder why the user or the developer input is involved in this, is because this series of articles about <code>RabbitMQ</code> is more educational rather than a solution-oriented approach. So I wanted to replicate one to one the <a href="http://tryrabbitmq.com/">http://tryrabbitmq.com/</a> concept of learning the basic principles of RabbitMQ using .NET.<br>Thus, after the connection to the RabbitMQ Server was built, the user is asked to enter the number of <code>Queues</code> that he wants to create and to provide for each <code>Queue</code> a name and the <code>routing key</code>.</p><script src="https://gist.github.com/StefanescuEduard/83a31ce8f1e15d6441190c10987765d2.js"></script><h3 id="Connection-establishment"><a href="#Connection-establishment" class="headerlink" title="Connection establishment"></a>Connection establishment</h3><p>When the required properties are entered, for each <code>Queue</code> a <code>Connection</code> and a communication <code>Channel</code> will be created. After that, the <code>Queue</code> is declared, and the bound to the <code>Exchange</code> that has the same name for the entire solution (i.e. test-exchange). If the creation and the binding were ended successfully, then a message is prompted informing the user that the <code>Queue</code> was created.</p><script src="https://gist.github.com/StefanescuEduard/685736ccd9d0045ad3cf89449d3278b5.js"></script><h4 id="QueueDeclare-parameters"><a href="#QueueDeclare-parameters" class="headerlink" title="QueueDeclare parameters"></a><code>QueueDeclare</code> parameters</h4><p>Let’s dig a little bit deep into the parameters of the <code>QueueDeclare</code>:</p><ul><li><code>queue</code> it’s self-evident that it refers to the <code>Queue</code> name;</li><li><code>durable</code> represents the lifetime of the <code>Queue</code> in the <code>Broker</code>, if it’s set to false the <code>Queue</code> will end when the <code>Broker</code> does too;</li><li><code>autoDelete</code> is used to specify the lifetime of the <code>Queue</code> based on its subscriptions. The <code>Queue</code> will be deleted if the last <code>Consumer</code> subscribed to it, unsubscribes;</li><li><code>arguments</code> are used to sent information about the <code>Queue</code> (e.g. length limit) to the <code>Broker</code>;</li></ul><p><code>QueueBind</code>‘s parameters are self-explanatory, the first one refers to the <code>queueName</code>, the second to the <code>exchangeName</code> and the last one to the <code>routingKey</code>.</p><h4 id="The-result"><a href="#The-result" class="headerlink" title="The result"></a>The result</h4><p>After the <code>Queues</code> are created, a message is prompted to the user with how many <code>Queues</code> were created.</p>    <div>      <img src="rabbitmq-queue-created-console.png" alt="RabbitMQ Queue Created Console" data-action="zoom" class="photozoom">          </div><p>The created <code>Queues</code> can also be seen on the RabbitMQ Management page.</p>    <div>      <img src="rabbitmq-queue-created-management.png" alt="RabbitMQ Queue Created Management" data-action="zoom" class="photozoom">          </div><p>Now, all we have to do is to create the <code>Consumer</code>, and the entire topology will be ready to be used. The <code>Consumer</code> will be created in the next article, this being the last one from this series.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> amqp </tag>
            
            <tag> docker </tag>
            
            <tag> queue </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ Exchange with Docker in .NET</title>
      <link href="2020/03/07/rabbitmq-exchange-with-docker-in-dotnet/"/>
      <url>2020/03/07/rabbitmq-exchange-with-docker-in-dotnet/</url>
      
        <content type="html"><![CDATA[<p>This is the second article from a series of four about RabbitMQ <code>Exchange</code> in .NET. In the first part of this article, the fundamentals and the types of the <code>Exchange</code> will be presented. And in the second part, each line of code will be explained about how the <code>Exchange</code> it will be created. The first article, <a href="https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/">https://stefanescueduard.github.io/2020/02/29/rabbitmq-producer-with-docker-in-dotnet/</a>, contains a short introduction into RabbitMQ concepts, the Environment setup with Docker and <code>Producer</code> creation in .NET.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The RabbitMQ <code>Exchange</code> is used to route the messages received from the <code>Producer</code> and sending them to the <code>Queue</code> based on the <code>routing key</code> key and <code>binding key</code>. Those two keys were explained in the first article, so if you want to find the explanation behind them, read the first article.<br>An <code>Exchange</code> can be one of these four types:</p><h3 id="Exchange-types"><a href="#Exchange-types" class="headerlink" title="Exchange types"></a><code>Exchange</code> types</h3><ol><li><p> <code>Direct Exchange</code> send messages to the <code>Queues</code> that have the <code>binding key</code> the same as the <code>routing key</code>.</p></li><li><p> <code>Fanout Exchange</code> send messages to all the <code>Queues</code>, in an indiscriminate way. It uses the Publish-Subscribe Pattern, which means that all <code>Queues</code> that are subscribed to an <code>Exchange</code> will receive the message, like a newsletter. Also, it will not take into account the <code>binding</code> or the <code>routing</code> key.</p></li><li><p><code>Topic Exchange</code> send messages to the <code>Queues</code> using a pattern. The pattern can contain <code>*</code> or <code>#</code>. The first wildcard <code>*</code> means that it can match any word, and the second one <code>#</code> represents that no word is expected in that place.</p><p> For example, if there are three <code>Queues</code>, and each one expects to get a ‘word’ based on the <code>binding key</code>, as follows:</p> <pre> Queue1 > word1._._ Queue2 > _.word2._ Queue3 > _._.word3 </pre><p> If the <code>Producer</code> sends the message <code>word1.word2.word3</code>, each <code>Queue</code> will receive only the expected message.</p><p> A real-life example for this type of <code>Exchange</code> can be the different levels of logging (i.e. the errors and warns can be sent to the Database, and the info ones sent to a file).</p></li><li><p> <code>Headers Exchange</code> send messages based on a header, this header is represented by a required property <code>x-match</code> and one or more properties that will be bound to the <code>Queue</code> like a <code>routing-key</code>. This type of <code>Exchange</code> is useful when the message should follow the header values and not the <code>routing keys</code>. It’s also associated with the <code>Topic Exchange</code>, but it’s much more flexible, with the disadvantage that the message should be an object.</p></li></ol><p>A topology can have multiple <code>Exchanges</code> connected to the multiple <code>Producers</code> and <code>Queues</code>, as in the example below, made using <a href="http://tryrabbitmq.com/">http://tryrabbitmq.com</a>.</p>    <div>      <img src="exchange-multiple-connections.png" alt="Exchange Multiple Connections" data-action="zoom" class="photozoom">          </div><p>Short explanation; the <code>Producers</code> can be seen as Clients, the <code>Queues</code> as Buffers or as Preprocessor and the <code>Consumers</code> are the applications that receive the messages. In this example the Clients can be considered as a mobile application that sent two requests to the server in order to persist some data, the Buffers are like a middleware which scan the data, and the <code>Consumers</code> are different storages that save the data based on its file type.</p><h2 id="Creating-the-Exchange"><a href="#Creating-the-Exchange" class="headerlink" title="Creating the Exchange"></a>Creating the <code>Exchange</code></h2><p>This project only covers the first tree <code>Exchange</code> types (i.e. Direct, Fanout and Topic), because the Headers type is a little more special I want to make another article just for it.</p><p>This, like the rest of the projects, is a Console Application. First of all, the user is asked which of <code>Exchange</code> wants to create:</p><script src="https://gist.github.com/StefanescuEduard/b97cae8daf78df512889292c15a1df93.js"></script><p>Then the connection to the RabbitMQ Server is created using the URI.</p><script src="https://gist.github.com/StefanescuEduard/e8570a4d26624b032c13558e22575d65.js"></script><p>In the first article, the connection is created by specifying each property needed. Here the <code>URI</code> consists of following parts: the first one is the protocol used, in this case, <code>AMQP</code>, then the <code>username</code> and the <code>password</code>, and the last section is the <code>hostname</code> or the <code>IP</code> of the RabbitMQ server. The <code>URI</code> can also contain the port, here the default one is 5672.<br>The <code>ConnectionTimeout</code> is set to the maximum value because the application is desired to run indefinitely, and this value is used to bypass the connection time out.</p><h3 id="Exchange-definition"><a href="#Exchange-definition" class="headerlink" title="Exchange definition"></a><code>Exchange</code> definition</h3><p>In order to establish the connection between our machine and the RabbitMQ Server, the stream must be opened, and through this stream, the information is sent using a Channel.</p><script src="https://gist.github.com/StefanescuEduard/28993423b205b545df54dca70d3c92a1.js"></script><p>The exchange name will have the same name for all four projects but is just for this article presentation purposes. On line 6 a safety deletion is executed in order, to ensure that on the next line when the Exchange will be created, there is no other <code>Exchange</code> with the same name. The <code>ExchangeDeclare</code> method will create an instance of an <code>Exchange</code> based on what type of <code>Exchange</code> the user has chosen.</p><h3 id="Prompting-the-definition-result"><a href="#Prompting-the-definition-result" class="headerlink" title="Prompting the definition result"></a>Prompting the definition result</h3><p>On the last section of the code, the user is informed that the required <code>Exchange</code> was created. You may notice that I didn’t do any type of exception handling, that’s because it’s only for this article presentation. But in a real-world application, I recommend to implement a strong exception handling system and make the other services involved aware if that happens.</p><h3 id="The-result"><a href="#The-result" class="headerlink" title="The result"></a>The result</h3><p>Let’s create a <code>Direct Exchange</code>, which will also appear on the RabbitMQ Management plugin.</p>    <div>      <img src="exchange-created-successfully.png" alt="Exchange Created Successfully" data-action="zoom" class="photozoom">          </div><p>And now let’s check the Management Exchange page.</p>    <div>      <img src="rabbitmq-management-exchange-created.png" alt="RabbitMQ Management Exchange Created" data-action="zoom" class="photozoom">          </div><p>As is showed in this page the <code>test-exchange</code> <code>Exchange</code> was created with the type that the user wanted and is ready to be used.</p><p>On the <code>test-exchange</code> dashboard, there a message can be published, as we did in the first article with the <code>Producer</code>. I will demonstrate both ways of publishing the messages in the last article, where the entire system will be connected. And also the <code>Exchange</code> can be deleted.</p>    <div>      <img src="test-exchange-page.png" alt="test-exchange page" data-action="zoom" class="photozoom">          </div><p>Because the topology currently doesn’t have connected all needed components, on the <code>Message rates</code> the <code>Idle</code> message is displayed, but we’ll see that after the entire system is running, there will be a chart that will show the messages flow, as Publish (In) and Publish (Out).</p><p>All the code from the entire solution is available on my Github account: <a href="https://github.com/StefanescuEduard/RabbitMQ_POC">https://github.com/StefanescuEduard/RabbitMQ_POC</a>. But stay close because in the next articles I will explain the core concepts about each RabbitMQ node.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> amqp </tag>
            
            <tag> docker </tag>
            
            <tag> exchange </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RabbitMQ Producer with Docker in .NET</title>
      <link href="2020/02/29/rabbitmq-producer-with-docker-in-dotnet/"/>
      <url>2020/02/29/rabbitmq-producer-with-docker-in-dotnet/</url>
      
        <content type="html"><![CDATA[<p>This is the first article from a series of four, where I will explain each RabbitMQ graph node, the environment setup with Docker and the <code>Producer</code> creation in .NET.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>RabbitMQ it’s a messaging system that uses AMQP (Advanced Message Queue Protocol) to deliver messages between communication levels. The main idea is that RabbitMQ uses the following 4 layers: <code>Producer</code>, <code>Exchange</code>, <code>Queue</code> and <code>Consumer</code> to send messages over AMQP.</p><p>The AMQP it’s delimited in the following three zones:</p><ol><li><p> The <code>Producer</code> sends a message to the <code>Message Broker</code>.</p></li><li><p> The <code>Message Broker</code> consists of at least one <code>Exchange</code> and at least one <code>Queue</code>. Then the <code>Exchange</code> sent the message to the <code>Queue</code>.</p></li><li><p> And the <code>Consumer</code> receives the message from the <code>Queue</code>.</p></li></ol><p>The <code>Exchange</code> can be one of four types in order to control the messages to the <code>Queues</code>. That’s a great way to handle messages when the system has multiple <code>Exchanges</code> and uses a <code>Queue</code> for multiple purposes (i.e. a Logger(<code>Producer</code>) send a log using a Service(<code>Exchange</code>) to a PersistenceAPI and an AuditAPI both using the same <code>Queue</code>, but both APIs saving the log in the same Database(<code>Consumer</code>)). Maybe that’s a silly example but I want to describe a possible way of using a single <code>Queue</code> with multiple <code>Exchanges</code>. And the can be easily reversed, the configuration can be whatever you wish, at least it will also have sense and respects the RabbitMQ’s concepts.</p><p>To understand how messages are sent in this system, firstly the roles of the <code>routing</code> and <code>binding</code> keys needs to be explained. So the <code>routing key</code> will be attached to message and sent to the <code>Exchange</code>, the <code>Exchange</code> will determine in which direction to send the received message based on its <code>routing key</code> and the <code>binding key</code> between the <code>Exchange</code> and the <code>Queue</code>. The <code>binding key</code> is also known as the routing pattern.</p><h3 id="Topology"><a href="#Topology" class="headerlink" title="Topology"></a>Topology</h3><p>In the image below is the topology created with <a href="http://tryrabbitmq.com/">http://tryrabbitmq.com</a>, which is a nice tool that helped me understand the core concepts of RabbitMQ.</p>    <div>      <img src="used-topology.png" alt="Topology" data-action="zoom" class="photozoom">          </div><p>This example will also be used to create the RabbitMQ topology using .NET Core.</p><h2 id="Docker-setup"><a href="#Docker-setup" class="headerlink" title="Docker setup"></a>Docker setup</h2><p>RabbitMQ can be installed locally without Docker, but I like to use Docker because is much faster and it’s a lot easier to make changes without breaking too many things.</p><p>In order to run RabbitMQ on Docker, we need the RabbitMQ Server and the Management Plugin, more information about the RabbitMQ images can be found here: <a href="https://hub.docker.com/_/rabbitmq">https://hub.docker.com/_/rabbitmq</a>.</p><p>Firstly the RabbitMQ image needs to be downloaded locally with the following command:</p><script src="https://gist.github.com/StefanescuEduard/fe30e3486887ae34592816e1c48fc7fb.js"></script><p>I recommend the Powershell or the Git Bash terminal to be used instead of CMD. This command will bring the latest version of RabbitMQ. To check that the image was pulled correctly, the <code>docker images</code> needs to be run. The following entry should be displayed after the command was executed:</p>    <div>      <img src="rabbitmq-image.png" alt="RabbitMQ Images" data-action="zoom" class="photozoom">          </div><p>After the image was downloaded, the RabbitMQ container can be created using the following command:</p><script src="https://gist.github.com/StefanescuEduard/c5623471e8afa23cda62256e5e2b91f9.js"></script><p>If you are new into the Docker world, this command will start an instance of the RabbitMQ image inside a container. And a container you can see it as a VirtualMachine that has all the packages needed to expose our required application.</p><p>The <code>--detach</code> parameter means that the container will run in the background, but we still can control it if is needed.</p><p>The <code>--name</code> parameter assign a name to the container to be easily identifiable. I choose the rabbitmq-blog name just for this article, but you can give it a name much more intuitive.</p><p>The <code>--publish</code> parameter will publish the container using the specified port, the first one is the port that we are expecting to access locally, and the second one is the container port. For this example, the container needs only two ports, the AMQP port (5672) and the Management port (15672). All available ports are listed here: <a href="https://www.rabbitmq.com/networking.html#ports">https://www.rabbitmq.com/networking.html#ports</a>. By default the Management plugin supports the <code>15672</code>, you can leave it like that or it can be changed, read more about this here: <a href="https://www.rabbitmq.com/management.html">https://www.rabbitmq.com/management.html</a>.</p><p>And the last argument is the RabbitMQ image. Using the colon (:) specify which version of that image to be used.</p><p>Multiple RabbitMQ containers can be linked using the network command:</p><script src="https://gist.github.com/StefanescuEduard/a892a3718e2c7888fc4e06f0ae2ffb45.js"></script><p>This command will create a network that can be used to link the containers and needs to be specified when the container is created, with the <code>--network</code> parameter in the <code>docker run</code> command.</p><p>To check that the containers are running, the <code>docker ps</code> command can be used. In the following image is the RabbitMQ container that was just started.</p>    <div>      <img src="rabbitmq-container.png" alt="RabbitMQ Container" data-action="zoom" class="photozoom">          </div><p>RabbitMQ Management it’s a nice tool to monitor and manage the entire topology, it can be accessed from <a href="http://localhost:8080/">http://localhost:8080</a>.</p>    <div>      <img src="rabbitmq-management-login.png" alt="RabbitMQ Management Login" data-action="zoom" class="photozoom">          </div><p>The credentials are <code>guest</code> and <code>guest</code>, those can be changed later.</p><p>After I will show how the <code>Exchange</code> can be created in .NET, I will return to the Management page, because the <code>Producer</code> can’t be seen on the Management page. The message published will be seen when will be received by the <code>Exchange</code>.</p><h2 id="Creating-the-Producer"><a href="#Creating-the-Producer" class="headerlink" title="Creating the Producer"></a>Creating the <code>Producer</code></h2><p>All the RabbitMQ nodes will be created as a Console App in .NET Core. The project only needs the <a href="https://www.nuget.org/packages/RabbitMQ.Client">RabbitMQ.Client</a> package available on nuget, but will also install some dependencies that are required as well.</p><h3 id="Setting-the-connection"><a href="#Setting-the-connection" class="headerlink" title="Setting the connection"></a>Setting the connection</h3><p>Firstly the connection to the RabbitMQ Server must be established using the container hostname and the default credentials.</p><script src="https://gist.github.com/StefanescuEduard/a065c80c82fd9f7a1e1b68fdf7885c00.js"></script><p>The <code>ConnectionTimeout</code> property is used to avoid the protocol operations time out, that’s why the following code is running in an infinite loop, I wanted to produce a message every time on a new or on the same <code>routing key</code>.</p><h3 id="User-interaction"><a href="#User-interaction" class="headerlink" title="User interaction"></a>User interaction</h3><p>Then, in the <code>while</code> loop, the user is constantly asked to introduce the <code>routing key</code> and the message that will be published.</p><script src="https://gist.github.com/StefanescuEduard/0f4ec653ae405a3ae9cd4c0b522b3d30.js"></script><h3 id="Creating-the-communication-channel"><a href="#Creating-the-communication-channel" class="headerlink" title="Creating the communication channel"></a>Creating the communication channel</h3><p>To create the connection between the <code>Exchange</code> and the <code>Producer</code>, the connection that was set earlier must be created and the communication channel needs to be opened, that’s what the first and the second line does.</p><script src="https://gist.github.com/StefanescuEduard/3e301b944e453a58e924a21d394a6077.js"></script><p>Then the <code>BasicProperties</code> are created for starting an elementary channel. These properties can be used to specify the <code>Persistence</code> type or the <code>Expiration</code> time until the message will be deleted if no <code>Consumer</code> will receive it; but there are many more and you can check them all here: <a href="https://www.rabbitmq.com/dotnet-api-guide.html">https://www.rabbitmq.com/dotnet-api-guide.html</a>.<br>On line 9 the message is published to the <code>Exchange</code> with the <code>routing key</code> and with the <code>BasicProperties</code> that were created. I preferred to choose a constant name for the <code>Exchange</code> just for this article, this <code>Exchange</code> name will be also used for the other nodes.</p><p>All code from this article can also be found on my Github account: <a href="https://github.com/StefanescuEduard/RabbitMQ_POC">https://github.com/StefanescuEduard/RabbitMQ_POC</a>. There is the entire Solution, but stay close, there will be an article with explanations for each project.<br>On the next article I will explain the <code>Exchange</code>.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> amqp </tag>
            
            <tag> docker </tag>
            
            <tag> producer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>.NET Core Code Coverage</title>
      <link href="2020/02/22/dotnet-core-code-coverage/"/>
      <url>2020/02/22/dotnet-core-code-coverage/</url>
      
        <content type="html"><![CDATA[<p>Code coverage tools are a great way to improve the code quality, but there are a lot of tools that require a paid license to use it like dotCover, the Enterprise version of Visual Studio or Visual Studio extensions. After some research, I found out that two free tools that combined can generate code coverage reports. The first tool is called <a href="https://github.com/tonerdo/coverlet">Coverlet</a> which generates the code coverage as I wanted and it’s also working with .NET Framework. Essentially is creating an <code>XML</code> report file that covers the lines, branches, and methods. And the second tool is <a href="https://github.com/danielpalme/ReportGenerator">ReportGenerator</a> which is used for parsing the generated <code>XML</code> and expose the data in a friendly format.</p><p>For this article, I created a small project to demonstrate how is working.<br>Let’s begin with the Business layer; which contains two classes, a dummy <code>User</code> that only has two properties <code>Roles</code> and <code>ManagedCountries</code>.</p><script src="https://gist.github.com/StefanescuEduard/dcbeb1fcdaa4e019c9764ab5ec8df0fb.js"></script><p>And a <code>UserService</code> that checks if the <code>User</code> has sufficient rights to delete a country.</p><script src="https://gist.github.com/StefanescuEduard/6e13369330c8e1a32d424da04d94ba2e.js"></script><p>I know that the return of the <code>CanDeleteCountry</code> method can be simplified, but I will keep as it is, just for the sake of this example.</p><p>The Tests project uses <code>NUnit</code>, but I saw that <code>Coverlet</code> and <code>ReportGenerator</code> also work with <code>MSBuild</code> and <code>xUnit</code>, for those you only need another TestAdapter and TestLogger. The packages installed for this project are:</p><ul><li><a href="https://www.nuget.org/packages/NUnit">NUnit</a></li><li><a href="https://www.nuget.org/packages/NUnit3TestAdapter">NUnit3TestAdapter</a></li><li><a href="https://www.nuget.org/packages/NunitXml.TestLogger">NunitXml.TestLogger</a></li><li><a href="https://www.nuget.org/packages/Microsoft.NET.Test.Sdk">Microsoft.NET.Test.Sdk</a></li><li><a href="https://www.nuget.org/packages/Microsoft.CodeCoverage">Microsoft.CodeCoverage</a></li><li><a href="https://www.nuget.org/packages/coverlet.msbuild">coverlet.msbuild</a></li><li><a href="https://www.nuget.org/packages/ReportGenerator">ReportGenerator</a></li></ul><p>In order to get the code coverage, a script is needed that will run the tests, create the <code>XML</code> file report and generate an <code>HTML</code> file with the reports.</p><script src="https://gist.github.com/StefanescuEduard/d7b91fa0c49c422b22b5911de2bd9566.js"></script><p>I’ve created a batch file to be more easy to run all the commands in a single unit. Also please note that if you are using Mac or Linux, the <code>nuget</code> packages path, it would be: <code>~/.nuget/packages</code>.<br>The first command will run the tests and will generate the <code>XML</code> file used by the second command to generate the report. And the third command will open <code>index.html</code>.<br>Please consider adding the <code>Coverage</code> folder into <code>.gitignore</code> if you are working on a team, this is just for local purposes and it would be nice if the project you are working on has on the CI build process a step where the code coverage is generated.</p><p>So now, with all that in place, let’s see the results. I wanted to test only if the <code>User</code> is not a <code>CountryManager</code> and skip the other conditions that will also check for the managed countries because I wanted to generate a report with all the available covered areas.</p><script src="https://gist.github.com/StefanescuEduard/efe808ad09dfbbd596fcc7afc2219f76.js"></script><p>Let’s take a look at the code coverage report. When the <code>index.html</code> file is open, the first page contains an overview of the ran tests, it offers a brief summary of the report and a grouping feature, that can be used to view the files on different layers (i.e. No grouping, By assembly, By namespace, Level: 1 and By namespace, Level: 2).</p>    <div>      <img src="code-coverage-overview.png" alt="Code coverage overview" data-action="zoom" class="photozoom">          </div><p>The test report for <code>UserService</code> class looks something like this:</p>    <div>      <img src="code-coverage-user-service-class.png" alt="Code coverage of UserService class" data-action="zoom" class="photozoom">          </div><p>The color code is the same as in the other tools, but what I like is that it also have the branch coverage, which in the above picture is the yellow area.</p><p>In conclusion, these two tools are great and don’t cost anything, also can be configured to run through all test projects and generate more comprehensive code coverage.</p><p>Here is the repository link with all the code from this article: <a href="https://github.com/StefanescuEduard/NetCoreCodeCoverage">https://github.com/StefanescuEduard/NetCoreCodeCoverage</a>. I kept the Coverage folder on the repository just for testing purposes.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> dotnetcore </tag>
            
            <tag> dotnet </tag>
            
            <tag> code coverage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Angular Ivy performance comparison</title>
      <link href="2020/02/19/angular-ivy-perfomance-comparison/"/>
      <url>2020/02/19/angular-ivy-perfomance-comparison/</url>
      
        <content type="html"><![CDATA[<p>Angular announces their new Ivy compiler, with smaller bundle sizes, so I wanted to find out on a real project how much the bundle size and the overall application performance were improved.</p><p>After seeing the Angular 9 release notes, I asked if the bundle size can be improved more. I found out that is possible by using compressing algorithms. After a little research, there are two libraries <a href="https://www.npmjs.com/package/zlib">zlib</a> and <a href="https://www.npmjs.com/package/brotli">brotli</a>, which run through all <code>.js</code>, <code>.html</code> and <code>.css</code> files and compress them using <code>gzip</code> and <code>brotli</code> compressing algorithms. I will create an article to show how this can be done.<br>But before showing the data, I want to present the project specifications. The project is a smaller one, with only 11 pages and it’s using Angular Material.<br>The test was run on a static page because I didn’t want to be affected by <code>HTTP</code> calls, and the browser used was Google Chrome. I tested the page size, the full load time of the page, time to build and the main bundle size. Another important thing that I want to mention is that the compressing algorithms were run through Angular 9 build files.</p><p>First, let’s take a look at the bundle size.</p>    <div>      <img src="main-bundle-size.png" alt="Main bundle size" data-action="zoom" class="photozoom">          </div><p>There is a huge difference between Angular 8 bundle size and Angular 9 with brotli, and we’ll also see that on page size comparison.</p>    <div>      <img src="page-size.png" alt="Page size" data-action="zoom" class="photozoom">          </div><p>The page size matches my expectations, it’s normal to have smaller sizes on the compressed files rather than uncompressed ones.<br>But let’s keep this in mind for a while, and let’s take look at the <strong>full page load</strong> time diagram, where the overall performance of this application was not significantly improved, and I will explain why.</p>    <div>      <img src="full-page-load.png" alt="Full page load" data-action="zoom" class="photozoom">          </div><p>As the diagram shows, the difference between Angular 8 and Angular 9 with the compression algorithms is only a few milliseconds, exactly 249ms, which is relatively OK for an application of this size, but I expected a little bit more also considering the results in the other diagrams.<br>The explanation for this is due to the Tree Shaking process, where the unused libraries are deleted, based on the tested project setup that doesn’t have a lot of imports and Tree Shaking didn’t have unwanted libraries to delete caused a very close time between the 4 samples. But the main factor of this smaller gaps is because of the project size and after that the Tree Shaking process.<br>Smaller page size means faster download which will decrease the page loading time. I will expect on a large application the time to be much more improved.</p>    <div>      <img src="time-to-build.png" alt="Time to build" data-action="zoom" class="photozoom">          </div><p>From a developer point of view, this is nice, Angular Ivy builds faster, and that is a nice advantage where the project is quite large. I want to mention that the times for the two compressing algorithm samples shouldn’t be here. In fact it’s the compressing time, it would have been unusual if the building plus the compressing time were smaller.<br/><br>Let’s calculate the building time for this two compressing algorithms:</p><h5 id="gzip"><a href="#gzip" class="headerlink" title="gzip:"></a>gzip:</h5><pre>Building time:    17481msCompressing time: 14913msTotal:            32394ms</pre><h5 id="brotli"><a href="#brotli" class="headerlink" title="brotli:"></a>brotli:</h5><pre>Building time:    14913msCompressing time: 14859msTotal:            29772ms</pre><p>The building time for Angular 8 was 29889ms, which means Angular 9 with brotli obtained lower building times. I know that is not a huge difference between these two, but it will scale up on larger projects.</p><p>In conclusion, the performance of Angular 9 is there and can be noticed on larger projects, still is a nice improvement compared to the previous Angular versions.</p><p>Thanks for reading this article, if you find it interesting please share it with your colleagues and friends. Or if you find something that can be improved please let me know.</p>]]></content>
      
      
      
        <tags>
            
            <tag> angular </tag>
            
            <tag> ivy </tag>
            
            <tag> performance </tag>
            
            <tag> benchmark </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
